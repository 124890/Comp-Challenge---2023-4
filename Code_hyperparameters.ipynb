{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must run from start using run all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "This section imports all libraries utilised within the programme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "# SKLearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Graphing\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from ipywidgets import interact, widgets, interactive\n",
    "\n",
    "\n",
    "'''Global variables\n",
    "    '''\n",
    "\n",
    "global gMethodDictionary\n",
    "global gVarErrorDf\n",
    "gVarErrorDf = pd.DataFrame(columns=[ 'Drop index','Predicted y', 'RMSE', 'R^2', 'MAE', 'RMSE %', 'R^2 %', 'MAE %'])\n",
    "gVarNames = ['Cement','Blast Furnace Slag','Fly Ash','Water','Superplasticizer','Coarse Aggregate','Fine Aggregate','Age','Concrete compressive strength',' ']\n",
    "gX_train_preprocessed = pd.DataFrame()\n",
    "gX_test_preprocessed = pd.DataFrame()\n",
    "gy_train = []\n",
    "gy_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_import():\n",
    "    \"\"\" \n",
    "    Import the CSV file and return the data as a pandas dataframe\n",
    "\n",
    "    Returns:\n",
    "    X: pandas dataframe\n",
    "    y: pandas dataframe\n",
    "    \"\"\"\n",
    "    #import data from the files\n",
    "    dataset = pd.read_csv('Concrete_Data_Yeh_final.csv')\n",
    "\n",
    "    #Data Preprocessing\n",
    "    #format as a dataframe\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    \n",
    "    #print(f'Null values: \\n',dataset.isnull().sum()) #check for null values\n",
    "    print(dataset.duplicated().sum(), 'duplicated rows dropped') #check for duplicates\n",
    "    dataset = dataset.drop_duplicates() #drop duplicates\n",
    "    dataset.dtypes #check for data types\n",
    "  \n",
    "    y = dataset[\"csMPa\"]\n",
    "    X = dataset.drop(\"csMPa\", axis=1)\n",
    "    \n",
    "    correlation_matrix = X.corr()\n",
    "    #export to excel\n",
    "    correlation_matrix.to_excel('correlation_matrix.xlsx')\n",
    "\n",
    "    # Plot the correlation matrix - Commented out as not needed in final run\n",
    "    #plt.figure(figsize=(10,10))\n",
    "    #sns.heatmap(correlation_matrix, annot=True, cmap=plt.cm.Reds)\n",
    "    #plt.show()\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X,y) -> None:\n",
    "    \"\"\" \n",
    "    Preprocess the data (simple imputer (mean), standard scaler) and split into training and test sets\n",
    "\n",
    "    Parameters:\n",
    "    X: pd.DataFrame\n",
    "    y: pd.DataFrame\n",
    "    \n",
    "    Returns: None (global variables are set)\n",
    "    \"\"\"\n",
    "    # Splitting the data into training and test sets\n",
    "    global gX_train_preprocessed, gX_test_preprocessed\n",
    "    global gy_train, gy_test    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Creating a preprocessing pipeline that imputes missing values with the mean \n",
    "    # and scales features to have zero mean and unit variance.\n",
    "    preprocessing_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    gX_train_preprocessed = pd.DataFrame(preprocessing_pipeline.fit_transform(X_train), columns=X_train.columns)\n",
    "    gX_test_preprocessed = pd.DataFrame(preprocessing_pipeline.transform(X_test), columns=X_test.columns)\n",
    "    gy_train = y_train\n",
    "    gy_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_regression(xTrainData: pd.DataFrame, yTrainData: pd.DataFrame, yTestData: pd.Series, bestFit = {})-> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function creates and fits a random forest regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (numpy.ndarray or pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (numpy.ndarray or pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (numpy.ndarray or pandas.DataFrame): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    y_pred (numpy.ndarray): The predicted values of compressive strength for the test set\n",
    "    \"\"\" \n",
    "   \n",
    "    regressor = RandomForestRegressor(**bestFit) # Creating the Random Forest Regressor\n",
    "    regressor.fit(xTrainData, yTrainData) \n",
    "    y_pred = regressor.predict(yTestData)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperperameters\n",
    "\n",
    "As shown Random Forrest Regression has the best untuned performance on our dataset and as such we are only analysing hyperperameters for this model:\n",
    "The most important hyperparameters to tune in a Random Forest model to improve its performance are:\n",
    "\n",
    "1. `n_estimators`: This is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees give you better performance but makes your code slower.\n",
    "\n",
    "2. `max_depth`: The maximum depth of the tree. This parameter can help to prevent overfitting. If the max depth is too high, the model may learn too much from the training data and perform poorly on unseen data.\n",
    "\n",
    "3. `min_samples_split`: The minimum number of samples required to split an internal node. If you increase this parameter, each tree in the forest becomes more constrained as it has to consider more samples at each node.\n",
    "\n",
    "4. `min_samples_leaf`: The minimum number of samples required to be at a leaf node. This parameter is similar to min_samples_splits, however, this describe the minimum number of samples of samples at the leafs, the base of the tree.\n",
    "\n",
    "5. `max_features`: The number of features to consider when looking for the best split. If set to \"auto\", then `max_features=sqrt(n_features)`.\n",
    "\n",
    "6. `bootstrap`: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n",
    "\n",
    "Both Random Search and Grid Search are hyperparameter tuning techniques, and each has its own advantages and disadvantages.\n",
    "\n",
    "**Grid Search** systematically works through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. The benefit is that it's guaranteed to find the best combination of parameters supplied. However, it can be computationally expensive, especially if the number of parameters or their possible values are large.\n",
    "\n",
    "**Random Search** sets up a grid of hyperparameter values and selects random combinations to train the model and score. The benefit is that it's not as computationally expensive as Grid Search, and you have more control over how long you want it to run for, as you can set the number of iterations. However, it's not guaranteed to find the best parameters.\n",
    "\n",
    "In practice, it's often recommended to start with Random Search to narrow down the possible range of values for each hyperparameter, and then use Grid Search within that range to find the best combination.\n",
    "\n",
    "So, neither is strictly \"better\" - the best choice depends on your specific situation, including the number of hyperparameters you need to tune, the number of possible values for each hyperparameter, and the computational resources you have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "def random_grid_search():\n",
    "    \"\"\"\n",
    "    This function performs a random grid search to identify the best hyperparameter combination for the Random Forest Regressor model.\n",
    "\n",
    "    The function first defines a hyperparameter grid, then performs a randomized search to identify the best hyperparameter combination. \n",
    "    It then defines a narrower hyperparameter grid around the best hyperparameter combination and performs a grid search to identify the \n",
    "    best hyperparameter combination. Finally, it uses the best hyperparameter combination to train the final Random Forest model.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    '''param_grid = {\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'bootstrap': [True, False]\n",
    "    }'''\n",
    "\n",
    "\n",
    "    param_random = {\n",
    "        'n_estimators': [50, 100, 125, 135, 145, 155, 170], # 7\n",
    "        'max_depth': [None, 20, 30, 40], #4\n",
    "        'min_samples_split': [ 2, 3,5, 10], #5\n",
    "        'min_samples_leaf': [ 1, 2, 5, 10],#4\n",
    "        'min_impurity_decrease': [0.0, 0.2, 0.4, 0.5],#4\n",
    "        'bootstrap': [False], #2\n",
    "        'max_features': ['auto', 'sqrt'] #2\n",
    "    }\n",
    "\n",
    "    #add a parameter to the param_random dictionary\n",
    "    #param_random['max_features'] = ['auto', 'sqrt']\n",
    "\n",
    "    # Step 1: Perform randomized search\n",
    "    rf_random = RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=param_random, n_iter=300, cv=5, n_jobs=-1, verbose=2)\n",
    "    rf_random.fit(gX_train_preprocessed, gy_train)\n",
    "\n",
    "    # Step 2: Identify the best hyperparameter combination from randomized search\n",
    "    best_random_params = rf_random.best_params_\n",
    "    print(\"Best Random Forrest Parameters:\", best_random_params)\n",
    "\n",
    "    # Step 3: Define a narrower hyperparameter grid around the best hyperparameter combination\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [best_random_params['n_estimators'] - 10, best_random_params['n_estimators'], best_random_params['n_estimators'] + 10],\n",
    "        'min_samples_split': [best_random_params['min_samples_split'] - 1 if best_random_params['min_samples_split'] > 1 else 2, best_random_params['min_samples_split'], best_random_params['min_samples_split'] + 1],\n",
    "        'min_samples_leaf': [best_random_params['min_samples_leaf'] - 1 if best_random_params['min_samples_leaf'] > 1 else 2, best_random_params['min_samples_leaf'], best_random_params['min_samples_leaf'] + 1],\n",
    "        'min_impurity_decrease': [best_random_params['min_impurity_decrease'] - 0.1 if best_random_params['min_impurity_decrease'] > 0.1 else 0, best_random_params['min_impurity_decrease'], best_random_params['min_impurity_decrease'] + 0.1],\n",
    "        'bootstrap': [best_random_params['bootstrap']],\n",
    "        'max_features': [best_random_params['max_features']]\n",
    "    }\n",
    "\n",
    "    if best_random_params['max_depth'] == None:\n",
    "        param_grid['max_depth'] = [None]\n",
    "    else:\n",
    "        param_grid['max_depth'] = [best_random_params['max_depth'] - 5, best_random_params['max_depth'], best_random_params['max_depth'] + 5]\n",
    "    \n",
    "\n",
    "    # Step 5: Perform grid search\n",
    "    rf_grid = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    rf_grid.fit(gX_train_preprocessed, gy_train)\n",
    "\n",
    "    # Step 6: Identify the best hyperparameter combination from grid search\n",
    "    best_grid_params = rf_grid.best_params_\n",
    "\n",
    "    # Use the best hyperparameter combination to train the final Random Forest model\n",
    "    final_rf_model = RandomForestRegressor(**best_grid_params)\n",
    "    final_rf_model.fit(gX_train_preprocessed, gy_train)\n",
    "    print(\"Best Random Forrest Parameters:\", best_grid_params)\n",
    "    return best_grid_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_unopt_random_forest(best_grid_params):\n",
    "    \"\"\"\n",
    "    This function compares the performance of the original unoptimized Random Forest Regressor model with the optimized Random Forest Regressor model.\n",
    "\n",
    "    The function first trains the original unoptimized Random Forest Regressor model and uses it to predict the target variable for the testing data. \n",
    "    It then trains the optimized Random Forest Regressor model and uses it to predict the target variable for the testing data. Finally, it calculates \n",
    "    the root mean squared error (RMSE), R^2 score, and mean absolute error (MAE) for the predictions of both models and prints the results.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\" \n",
    "    # Original unoptimized Random Forest Regression\n",
    "    y_pred_unoptimized = random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed)\n",
    "\n",
    "    # Optimized Random Forest Regression  \n",
    "    y_pred_optimized = random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed,best_grid_params)\n",
    "\n",
    "    # Calculate errors\n",
    "    rmse_unoptimized = sqrt(mean_squared_error(gy_test, y_pred_unoptimized))\n",
    "    rmse_optimized = sqrt(mean_squared_error(gy_test, y_pred_optimized))\n",
    "    r2_unoptimized = r2_score(gy_test, y_pred_unoptimized)\n",
    "    r2_optimized = r2_score(gy_test, y_pred_optimized)\n",
    "    mae_unoptimized = mean_absolute_error(gy_test, y_pred_unoptimized)\n",
    "    mae_optimized = mean_absolute_error(gy_test, y_pred_optimized)\n",
    "\n",
    "    # Print errors\n",
    "    print(\"Unoptimized RMSE:\", rmse_unoptimized)\n",
    "    print(\"Optimized RMSE:\", rmse_optimized)\n",
    "    print(\"Unoptimized R^2:\", r2_unoptimized)\n",
    "    print(\"Optimized R^2:\", r2_optimized)\n",
    "    print(\"Unoptimized MAE:\", mae_unoptimized)\n",
    "    print(\"Optimized MAE:\", mae_optimized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: main()\n",
    "\n",
    "The `main()` function is the entry point of the program. It is responsible for coordinating the execution of other functions and controlling the flow of the program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 duplicated rows dropped\n",
      "Preprocessing done\n",
      "Performing grid search \n",
      "\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Best Random Forrest Parameters: {'n_estimators': 170, 'min_samples_split': 3, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'sqrt', 'max_depth': 40, 'bootstrap': False}\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Best Random Forrest Parameters: {'bootstrap': False, 'max_depth': 45, 'max_features': 'sqrt', 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 180}\n",
      "********Time taken for grid search: 145.60913610458374 seconds********\n",
      "\n",
      "Grid search complete. Errors are as follows:\n",
      "Unoptimized RMSE: 4.714286780772993\n",
      "Optimized RMSE: 3.95954740828568\n",
      "Unoptimized R^2: 0.9030608116870117\n",
      "Optimized R^2: 0.9316153738265277\n",
      "Unoptimized MAE: 3.5476632626119753\n",
      "Optimized MAE: 2.9110781536486945\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def main():\n",
    "    X,y = csv_import()\n",
    "    preprocessing(X,y)\n",
    "    print(\"Preprocessing done\")\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    print(\"Performing grid search \\n\")\n",
    "    start_time = time.time()  # Start the timer\n",
    "    best_grid_params = random_grid_search()\n",
    "    end_time = time.time()  # End the timer\n",
    "    print(\"********Time taken for grid search:\", end_time - start_time, \"seconds********\\n\")\n",
    "    print(\"Grid search complete. Errors are as follows:\")\n",
    "    opt_unopt_random_forest(best_grid_params)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
