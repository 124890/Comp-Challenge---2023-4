{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code_hyperparameters\n",
    "\n",
    "- This code was used for running external extensive hyperparameter testing to inform our choices over parameter grids and optimisation methods\n",
    "- We have included this code to show the larger parameter grids we chose to test, running for >1hr using randomised and overnight when using grid search\n",
    "\n",
    "- As stated in our main code, we use a **2 step optimisation process** increasing efficiency and the likleyhood of finding the \"best\" parameter combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "This section imports all libraries utilised within the programme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "# SKLearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "#Define global variables\n",
    "global gMethodDictionary\n",
    "global gVarErrorDf\n",
    "gVarErrorDf = pd.DataFrame(columns=[ 'Drop index','Predicted y', 'RMSE', 'R^2', 'MAE', 'RMSE %', 'R^2 %', 'MAE %'])\n",
    "gVarNames = ['Cement','Blast Furnace Slag','Fly Ash','Water','Superplasticizer','Coarse Aggregate','Fine Aggregate','Age','Concrete compressive strength',' ']\n",
    "gX_train_preprocessed = pd.DataFrame()\n",
    "gX_test_preprocessed = pd.DataFrame()\n",
    "gy_train = []\n",
    "gy_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_import()->pd.DataFrame:\n",
    "    \"\"\" \n",
    "    Import the CSV file and return the data as a pandas dataframe\n",
    "\n",
    "    Returns:\n",
    "    X_dataset: (pd.DataFrame): Feature variables (Cement, Blast Furnace Slag, Fly Ash, Water, Superplasticizer, Coarse Aggregate, Fine Aggregate, Age)\n",
    "    y_dataset (pd.Series): Target variable (Concrete compressive strength)\n",
    "    \"\"\"\n",
    "    #import data from the files\n",
    "    dataset = pd.read_csv('Concrete_Data_Yeh_final.csv')\n",
    "\n",
    "    #Data Preprocessing\n",
    "    #format as a dataframe\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    \n",
    "    #print(f'Null values: \\n',dataset.isnull().sum()) #check for null values\n",
    "    print(dataset.duplicated().sum(), 'duplicated rows dropped') #check for duplicates\n",
    "    dataset = dataset.drop_duplicates() #drop duplicates\n",
    "    dataset.dtypes #check for data types\n",
    "  \n",
    "    y_dataset = dataset[\"csMPa\"]\n",
    "    X_dataset = dataset.drop(\"csMPa\", axis=1)\n",
    "    return X_dataset, y_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_dataset: pd.DataFrame, y_dataset: pd.Series) -> None:\n",
    "    \"\"\" \n",
    "    Preprocess the data (simple imputer (mean), standard scaler) and split into training and test sets\n",
    "\n",
    "    Parameters:\n",
    "    X_dataset (pd.DataFrame): The feature variables\n",
    "    y_dataset (pd.Series): The target variable (concrete compressive strength)\n",
    "    \n",
    "    Returns: None (global variables are set)\n",
    "    \"\"\"\n",
    "    # Splitting the data into training and test sets\n",
    "    global gX_train_preprocessed, gX_test_preprocessed\n",
    "    global gy_train, gy_test    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Creating a preprocessing pipeline that imputes missing values with the mean \n",
    "    # and scales features to have zero mean and unit variance.\n",
    "    preprocessing_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    gX_train_preprocessed = pd.DataFrame(preprocessing_pipeline.fit_transform(X_train), columns=X_train.columns)\n",
    "    gX_test_preprocessed = pd.DataFrame(preprocessing_pipeline.transform(X_test), columns=X_test.columns)\n",
    "    gy_train = y_train\n",
    "    gy_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_regression(xTrainData: pd.DataFrame, yTrainData: pd.DataFrame, yTestData: pd.Series, bestFit = {})-> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function creates and fits a random forest regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (pandas.Series): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    y_pred (numpy.ndarray): The predicted values of compressive strength for the test set\n",
    "    \"\"\" \n",
    "   \n",
    "    regressor = RandomForestRegressor(**bestFit) # Creating the Random Forest Regressor\n",
    "    regressor.fit(xTrainData, yTrainData) \n",
    "    y_pred = regressor.predict(yTestData)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "As shown Random Forest Regression has the best untuned performance on our dataset and as such we are only analysing hyperparameters for this model:\n",
    "The most important hyperparameters to tune in a Random Forest model to improve its performance are:\n",
    "\n",
    "1. `n_estimators`: Higher number of trees give you better performance but is more computationally intense.\n",
    "\n",
    "2. `max_depth`: Can help to prevent overfitting. If the max depth is too high, the model may learn too much from the training data and perform poorly on test data.\n",
    "\n",
    "3. `min_samples_split`: Increasing this parameter increases the number of samples considered at each node.\n",
    "\n",
    "4. `min_samples_leaf`: Similar to min_samples_splits, instead describing the minimum number of samples of samples at the leafs\n",
    "\n",
    "5. `max_features`: The number of features to consider when looking for the best split. When using \"auto\", `max_features=sqrt(n_features)`.\n",
    "\n",
    "6. `bootstrap`: If False, the whole dataset is used to build each tree. We chose to only use False throught as preforming initial testing found true to yield significantly lower performance\n",
    "\n",
    "We chose to test these along with minimum impurity decrease.\n",
    "\n",
    "**Random Search** sets up a grid of hyperparameter values and selects random combinations and is therefore less computationally expensive then Grid Search (and was found to be better optimised for multi-core processing). We use this step first, as while efficient it's not guaranteed to find the best parameters.\n",
    "\n",
    "**Grid Search** systematically works through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. We use a smaller grid defined around the results of the random search.\n",
    "\n",
    "**NOTE: This shows are larger test grid - this config took ~1hr30 to run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_grid_search():\n",
    "    \"\"\"\n",
    "    This function performs a randomised search to narrow down the hyperparameter grid for the Random Forest Regressor.\n",
    "    It then performs a grid search based on the randomised search results, to find the best hyperparameter combination.\n",
    "\n",
    "    Parameters: None\n",
    "    Returns: best_grid_params (dict): The best hyperparameter combination\n",
    "    \"\"\"\n",
    "\n",
    "    # Large hyperparameter grid for randomised search - 13*7*5*5*5*1*2 = 22,750 combinations\n",
    "    # While this is a large run during testing we completed several larger runs including using full exhaustive grid search over the entire hyperparameter space\n",
    "    param_random = {\n",
    "        'n_estimators': [50, 100, 125, 135, 145, 155, 170, 180, 190, 200, 300, 400,600], \n",
    "        'max_depth': [None, 20, 30, 40, 60, 80, 100],\n",
    "        'min_samples_split': [ 2, 3,5, 10, 20], \n",
    "        'min_samples_leaf': [ 1, 2, 5, 10, 20],\n",
    "        'min_impurity_decrease': [0.0, 0.2, 0.4, 0.5, 0.8],\n",
    "        'bootstrap': [False], \n",
    "        'max_features': ['auto', 'sqrt'] \n",
    "    }\n",
    "\n",
    "\n",
    "    # Randomised search - 20,000 iterations\n",
    "    rf_random = RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=param_random, n_iter=20000, cv=5, n_jobs=-1, verbose=2)\n",
    "    rf_random.fit(gX_train_preprocessed, gy_train)\n",
    "\n",
    "    # Finding the best hyperparameter combination from randomised search\n",
    "    best_random_params = rf_random.best_params_\n",
    "    print(\"Best Random Forrest Parameters:\", best_random_params)\n",
    "\n",
    "    # Defining a smaller hyperparameter grid for grid search\n",
    "    param_grid = {\n",
    "        'n_estimators': [best_random_params['n_estimators'] - 10, best_random_params['n_estimators'], best_random_params['n_estimators'] + 10],\n",
    "        'min_samples_split': [best_random_params['min_samples_split'] - 1 if best_random_params['min_samples_split'] > 1 else 2, best_random_params['min_samples_split'], best_random_params['min_samples_split'] + 1],\n",
    "        'min_samples_leaf': [best_random_params['min_samples_leaf'] - 1 if best_random_params['min_samples_leaf'] > 1 else 2, best_random_params['min_samples_leaf'], best_random_params['min_samples_leaf'] + 1],\n",
    "        'min_impurity_decrease': [best_random_params['min_impurity_decrease'] - 0.1 if best_random_params['min_impurity_decrease'] > 0.1 else 0, best_random_params['min_impurity_decrease'], best_random_params['min_impurity_decrease'] + 0.1],\n",
    "        'bootstrap': [best_random_params['bootstrap']],\n",
    "        'max_features': [best_random_params['max_features']]\n",
    "    }\n",
    "    # If statements used to ensure that hyperparameter values are not out of range (and within the param_grid)\n",
    "    if best_random_params['max_depth'] == None:\n",
    "        param_grid['max_depth'] = [None]\n",
    "    else:\n",
    "        param_grid['max_depth'] = [best_random_params['max_depth'] - 5, best_random_params['max_depth'], best_random_params['max_depth'] + 5]\n",
    "    \n",
    "    # Grid search\n",
    "    rf_grid = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    rf_grid.fit(gX_train_preprocessed, gy_train)\n",
    "\n",
    "    # Finding the best hyperparameter combination from grid search\n",
    "    best_grid_params = rf_grid.best_params_\n",
    "\n",
    "    # Use the best hyperparameter combination to train the final Random Forest model\n",
    "    final_rf_model = RandomForestRegressor(**best_grid_params)\n",
    "    final_rf_model.fit(gX_train_preprocessed, gy_train)\n",
    "    print(\"Best Random Forrest Parameters:\", best_grid_params)\n",
    "    return best_grid_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_unopt_random_forest(best_grid_params: dict):\n",
    "    \"\"\"\n",
    "    This function compares the performance of the original unoptimised Random Forest Regressor model with the optimised Random Forest Regressor model.\n",
    "    The model trained, using the hyperparameter combination identified by the grid search, performance metrics are compared to the original model. If the\n",
    "    optimised model performs worse than the original model, the grid search is deemed unsuccessful and an empty dictionary is returned.\n",
    "\n",
    "    Parameters: \n",
    "    best_grid_params (dict): The best hyperparameter combination from the grid search.\n",
    "    \n",
    "    Returns:\n",
    "    best_grid_params (dict): The best hyperparameter combination in case the optimisation was unsuccessful.\n",
    "    \"\"\" \n",
    "    \n",
    "    # Original unoptimized Random Forest Regression\n",
    "    y_pred_unoptimised = random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed)\n",
    "\n",
    "    # Optimised Random Forest Regressor\n",
    "    y_pred_optimised = random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed,best_grid_params)\n",
    "\n",
    "    # Calculate error matrics\n",
    "    rmse_unoptimised = sqrt(mean_squared_error(gy_test, y_pred_unoptimised))\n",
    "    rmse_optimised = sqrt(mean_squared_error(gy_test, y_pred_optimised))\n",
    "    r2_unoptimised = r2_score(gy_test, y_pred_unoptimised)\n",
    "    r2_optimised = r2_score(gy_test, y_pred_optimised)\n",
    "    mae_unoptimised = mean_absolute_error(gy_test, y_pred_unoptimised)\n",
    "    mae_optimised = mean_absolute_error(gy_test, y_pred_optimised)\n",
    "\n",
    "    # Print errors\n",
    "    print(\"Unoptimised RMSE:\", rmse_unoptimised)\n",
    "    print(\"Optimised RMSE:\", rmse_optimised)\n",
    "    print(\"Unoptimised R^2:\", r2_unoptimised)\n",
    "    print(\"Optimised R^2:\", r2_optimised)\n",
    "    print(\"Unoptimised MAE:\", mae_unoptimised)\n",
    "    print(\"Optimised MAE:\", mae_optimised)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: main()\n",
    "\n",
    "- The `main()` function is the entry point of the program. It is responsible for coordinating the execution of other functions and controlling the flow of the program.\n",
    "- This is a *\"stripped back\"* version from the main code only running the functions needed for hyperparameter optimisation \n",
    "\n",
    "- A timer is included in the function to determine how long the hyperparameter optimisation run took when run overnight/unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 duplicated rows dropped\n",
      "Preprocessing done\n",
      "Performing grid search \n",
      "\n",
      "Fitting 5 folds for each of 20000 candidates, totalling 100000 fits\n",
      "Best Random Forrest Parameters: {'n_estimators': 180, 'min_samples_split': 3, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'sqrt', 'max_depth': None, 'bootstrap': False}\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Best Random Forrest Parameters: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 180}\n",
      "********Time taken for grid search: 6757.785653829575 seconds********\n",
      "\n",
      "Grid search complete. Errors are as follows:\n",
      "Unoptimized RMSE: 4.7741369547647965\n",
      "Optimized RMSE: 4.093477595057695\n",
      "Unoptimized R^2: 0.9005838067551077\n",
      "Optimized R^2: 0.9269109666398374\n",
      "Unoptimized MAE: 3.5635091643092873\n",
      "Optimized MAE: 2.967129675467544\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    X_dataset, y_dataset = csv_import()\n",
    "    preprocessing(X_dataset, y_dataset)\n",
    "    print(\"Preprocessing done\")\n",
    "    \n",
    "    # Hyperparameter optimisation\n",
    "    print(\"Performing grid search \\n\")\n",
    "    start_time = time.time()  # Start the timer\n",
    "    best_grid_params = random_grid_search()\n",
    "    end_time = time.time()  # End the timer\n",
    "    print(\"********Time taken for grid search:\", end_time - start_time, \"seconds********\\n\")\n",
    "    print(\"Grid search complete. Errors are as follows:\")\n",
    "    opt_unopt_random_forest(best_grid_params) # Calculate errors for optimised and unoptimised models and print\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
