{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Must run from start using run all:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "This section imports all libraries utilised within the programme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "# SKLearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Graphing\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from ipywidgets import interact, widgets, interactive\n",
    "\n",
    "\n",
    "'''Global variables\n",
    "    '''\n",
    "\n",
    "global gMethodDictionary\n",
    "global gVarErrorDf\n",
    "gVarErrorDf = pd.DataFrame(columns=[ 'Drop index','Predicted y', 'RMSE', 'R^2', 'MAE', 'RMSE %', 'R^2 %', 'MAE %'])\n",
    "gVarNames = ['Cement','Blast Furnace Slag','Fly Ash','Water','Superplasticizer','Coarse Aggregate','Fine Aggregate','Age','Concrete compressive strength',' ']\n",
    "gX_train_preprocessed = pd.DataFrame()\n",
    "gX_test_preprocessed = pd.DataFrame()\n",
    "gy_train = []\n",
    "gy_test = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperperameters\n",
    "\n",
    "As shown Random Forrest Regression has the best untuned performance on our dataset and as such we are only analysing hyperperameters for this model:\n",
    "The most important hyperparameters to tune in a Random Forest model to improve its performance are:\n",
    "\n",
    "1. `n_estimators`: This is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees give you better performance but makes your code slower.\n",
    "\n",
    "2. `max_depth`: The maximum depth of the tree. This parameter can help to prevent overfitting. If the max depth is too high, the model may learn too much from the training data and perform poorly on unseen data.\n",
    "\n",
    "3. `min_samples_split`: The minimum number of samples required to split an internal node. If you increase this parameter, each tree in the forest becomes more constrained as it has to consider more samples at each node.\n",
    "\n",
    "4. `min_samples_leaf`: The minimum number of samples required to be at a leaf node. This parameter is similar to min_samples_splits, however, this describe the minimum number of samples of samples at the leafs, the base of the tree.\n",
    "\n",
    "5. `max_features`: The number of features to consider when looking for the best split. If set to \"auto\", then `max_features=sqrt(n_features)`.\n",
    "\n",
    "6. `bootstrap`: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n",
    "\n",
    "Both Random Search and Grid Search are hyperparameter tuning techniques, and each has its own advantages and disadvantages.\n",
    "\n",
    "**Grid Search** systematically works through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. The benefit is that it's guaranteed to find the best combination of parameters supplied. However, it can be computationally expensive, especially if the number of parameters or their possible values are large.\n",
    "\n",
    "**Random Search** sets up a grid of hyperparameter values and selects random combinations to train the model and score. The benefit is that it's not as computationally expensive as Grid Search, and you have more control over how long you want it to run for, as you can set the number of iterations. However, it's not guaranteed to find the best parameters.\n",
    "\n",
    "In practice, it's often recommended to start with Random Search to narrow down the possible range of values for each hyperparameter, and then use Grid Search within that range to find the best combination.\n",
    "\n",
    "So, neither is strictly \"better\" - the best choice depends on your specific situation, including the number of hyperparameters you need to tune, the number of possible values for each hyperparameter, and the computational resources you have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "def random_grid_search():\n",
    "    \"\"\"\n",
    "    This function performs a random grid search to identify the best hyperparameter combination for the Random Forest Regressor model.\n",
    "\n",
    "    The function first defines a hyperparameter grid, then performs a randomized search to identify the best hyperparameter combination. \n",
    "    It then defines a narrower hyperparameter grid around the best hyperparameter combination and performs a grid search to identify the \n",
    "    best hyperparameter combination. Finally, it uses the best hyperparameter combination to train the final Random Forest model.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    '''param_grid = {\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'bootstrap': [True, False]\n",
    "    }'''\n",
    "\n",
    "\n",
    "    param_random = {\n",
    "        'n_estimators': [50, 100, 125, 135, 145, 155, 170], # 7\n",
    "        'max_depth': [None, 20, 30, 40], #4\n",
    "        'min_samples_split': [ 2, 3,5, 10], #5\n",
    "        'min_samples_leaf': [ 1, 2, 5, 10],#4\n",
    "        'min_impurity_decrease': [0.0, 0.2, 0.4, 0.5],#4\n",
    "        'bootstrap': [False], #2\n",
    "        'max_features': ['auto', 'sqrt'] #2\n",
    "    }\n",
    "\n",
    "    #add a parameter to the param_random dictionary\n",
    "    #param_random['max_features'] = ['auto', 'sqrt']\n",
    "\n",
    "    # Step 1: Perform randomized search\n",
    "    rf_random = RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=param_random, n_iter=300, cv=5, n_jobs=-1, verbose=2)\n",
    "    rf_random.fit(gX_train_preprocessed, gy_train)\n",
    "\n",
    "    # Step 2: Identify the best hyperparameter combination from randomized search\n",
    "    best_random_params = rf_random.best_params_\n",
    "    print(\"Best Random Forrest Parameters:\", best_random_params)\n",
    "\n",
    "    # Step 3: Define a narrower hyperparameter grid around the best hyperparameter combination\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [best_random_params['n_estimators'] - 10, best_random_params['n_estimators'], best_random_params['n_estimators'] + 10],\n",
    "        'min_samples_split': [best_random_params['min_samples_split'] - 1 if best_random_params['min_samples_split'] > 1 else 2, best_random_params['min_samples_split'], best_random_params['min_samples_split'] + 1],\n",
    "        'min_samples_leaf': [best_random_params['min_samples_leaf'] - 1 if best_random_params['min_samples_leaf'] > 1 else 2, best_random_params['min_samples_leaf'], best_random_params['min_samples_leaf'] + 1],\n",
    "        'min_impurity_decrease': [best_random_params['min_impurity_decrease'] - 0.1 if best_random_params['min_impurity_decrease'] > 0.1 else 0, best_random_params['min_impurity_decrease'], best_random_params['min_impurity_decrease'] + 0.1],\n",
    "        'bootstrap': [best_random_params['bootstrap']],\n",
    "        'max_features': [best_random_params['max_features']]\n",
    "    }\n",
    "\n",
    "    if best_random_params['max_depth'] == None:\n",
    "        param_grid['max_depth'] = [None]\n",
    "    else:\n",
    "        param_grid['max_depth'] = [best_random_params['max_depth'] - 5, best_random_params['max_depth'], best_random_params['max_depth'] + 5]\n",
    "    \n",
    "\n",
    "    # Step 5: Perform grid search\n",
    "    rf_grid = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    rf_grid.fit(gX_train_preprocessed, gy_train)\n",
    "\n",
    "    # Step 6: Identify the best hyperparameter combination from grid search\n",
    "    best_grid_params = rf_grid.best_params_\n",
    "\n",
    "    # Use the best hyperparameter combination to train the final Random Forest model\n",
    "    final_rf_model = RandomForestRegressor(**best_grid_params)\n",
    "    final_rf_model.fit(gX_train_preprocessed, gy_train)\n",
    "    print(\"Best Random Forrest Parameters:\", best_grid_params)\n",
    "    return best_grid_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_unopt_random_forest(best_grid_params):\n",
    "    \"\"\"\n",
    "    This function compares the performance of the original unoptimized Random Forest Regressor model with the optimized Random Forest Regressor model.\n",
    "\n",
    "    The function first trains the original unoptimized Random Forest Regressor model and uses it to predict the target variable for the testing data. \n",
    "    It then trains the optimized Random Forest Regressor model and uses it to predict the target variable for the testing data. Finally, it calculates \n",
    "    the root mean squared error (RMSE), R^2 score, and mean absolute error (MAE) for the predictions of both models and prints the results.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\" \n",
    "    # Original unoptimized Random Forest Regression\n",
    "    y_pred_unoptimized = random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed)\n",
    "\n",
    "    # Optimized Random Forest Regression  \n",
    "    y_pred_optimized = random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed,best_grid_params)\n",
    "\n",
    "    # Calculate errors\n",
    "    rmse_unoptimized = sqrt(mean_squared_error(gy_test, y_pred_unoptimized))\n",
    "    rmse_optimized = sqrt(mean_squared_error(gy_test, y_pred_optimized))\n",
    "    r2_unoptimized = r2_score(gy_test, y_pred_unoptimized)\n",
    "    r2_optimized = r2_score(gy_test, y_pred_optimized)\n",
    "    mae_unoptimized = mean_absolute_error(gy_test, y_pred_unoptimized)\n",
    "    mae_optimized = mean_absolute_error(gy_test, y_pred_optimized)\n",
    "\n",
    "    # Print errors\n",
    "    print(\"Unoptimized RMSE:\", rmse_unoptimized)\n",
    "    print(\"Optimized RMSE:\", rmse_optimized)\n",
    "    print(\"Unoptimized R^2:\", r2_unoptimized)\n",
    "    print(\"Optimized R^2:\", r2_optimized)\n",
    "    print(\"Unoptimized MAE:\", mae_unoptimized)\n",
    "    print(\"Optimized MAE:\", mae_optimized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: main()\n",
    "\n",
    "The `main()` function is the entry point of the program. It is responsible for coordinating the execution of other functions and controlling the flow of the program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values: \n",
      " cement               0\n",
      "slag                 6\n",
      "flyash               1\n",
      "water                8\n",
      "superplasticizer    14\n",
      "coarseaggregate      7\n",
      "fineaggregate        3\n",
      "age                  5\n",
      "csMPa                0\n",
      "dtype: int64\n",
      "Duplicated values: 23\n",
      "Preprocessing done\n",
      "Performing grid search \n",
      "\n",
      "Fitting 5 folds for each of 300 candidates, totalling 1500 fits\n",
      "Best Random Forrest Parameters: {'n_estimators': 100, 'min_samples_split': 3, 'min_samples_leaf': 1, 'min_impurity_decrease': 0.0, 'max_features': 'sqrt', 'max_depth': 30, 'bootstrap': False}\n",
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Best Random Forrest Parameters: {'bootstrap': False, 'max_depth': 35, 'max_features': 'sqrt', 'min_impurity_decrease': 0, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 110}\n",
      "********Time taken for grid search: 101.90292930603027 seconds********\n",
      "\n",
      "Grid search complete. Errors are as follows:\n",
      "Unoptimized RMSE: 4.813945610963076\n",
      "Optimized RMSE: 4.015334217741445\n",
      "Unoptimized R^2: 0.898918950793743\n",
      "Optimized R^2: 0.9296748313554926\n",
      "Unoptimized MAE: 3.64978716289486\n",
      "Optimized MAE: 2.9422035703570355\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def main():\n",
    "    X,y = csv_import()\n",
    "    preprocessing(X,y)\n",
    "    print(\"Preprocessing done\")\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    print(\"Performing grid search \\n\")\n",
    "    start_time = time.time()  # Start the timer\n",
    "    best_grid_params = random_grid_search()\n",
    "    end_time = time.time()  # End the timer\n",
    "    print(\"********Time taken for grid search:\", end_time - start_time, \"seconds********\\n\")\n",
    "    print(\"Grid search complete. Errors are as follows:\")\n",
    "    opt_unopt_random_forest(best_grid_params)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
