{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "\n",
    "    This section imports all libraries utilised within the programme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File importing and preprocessing data\n",
    "    This section imports all libraries utilised within the programme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cement   slag  flyash  water  superplasticizer  coarseaggregate  \\\n",
      "0   540.0    0.0     0.0  162.0               2.5           1040.0   \n",
      "1   540.0    0.0     0.0  162.0               2.5           1055.0   \n",
      "2   332.5  142.5     0.0  228.0               0.0            932.0   \n",
      "3   332.5  142.5     0.0  228.0               0.0            932.0   \n",
      "4   198.6  132.4     0.0  192.0               0.0            978.4   \n",
      "\n",
      "   fineaggregate    age  csMPa  \n",
      "0          676.0   28.0  79.99  \n",
      "1          676.0   28.0  61.89  \n",
      "2          594.0  270.0  40.27  \n",
      "3          594.0    NaN  41.05  \n",
      "4          825.5  360.0  44.30  \n"
     ]
    }
   ],
   "source": [
    "#import data from the files\n",
    "dataset = pd.read_csv('Concrete_Data_Yeh_final.csv')\n",
    "\n",
    "#Data Preprocessing\n",
    "#format as a dataframe\n",
    "dataset = pd.DataFrame(dataset)#\n",
    "#check for null values\n",
    "dataset.isnull().sum()\n",
    "#check for duplicates\n",
    "dataset.duplicated().sum()\n",
    "#check for data types\n",
    "dataset.dtypes\n",
    "\n",
    "print(dataset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 480.     0.     0.  ...  936.   721.    28. ]\n",
      " [ 375.     0.     0.  ... 1038.   758.    28. ]\n",
      " [ 303.6  139.9    0.  ...  895.5  722.5   28. ]\n",
      " ...\n",
      " [ 144.     0.   175.  ...  943.   844.    28. ]\n",
      " [ 239.6  359.4    0.  ...  941.6  664.3   28. ]\n",
      " [ 192.   288.     0.  ...  929.8  716.1   90. ]]\n"
     ]
    }
   ],
   "source": [
    "#split into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 8].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression functions\n",
    "\n",
    "    This section defines functions for various regression functions which are later compared to choose the most effective model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a regressor using sci-kit learn functionalities, including the rational choice of a regression algorithm, data splitting, training, testing, and analysis of the hyper-parameter choices. Justify the choice of algorithm and parameters with an analysis of their effect.\n",
    "def linear_regression(X, y):\n",
    "    # Splitting the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "    # Creating the Linear Regression model\n",
    "    regressor = LinearRegression()\n",
    "\n",
    "    # Fitting the data\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the data\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    # Calculating the r2 score\n",
    "    print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "    # Calculating the mean squared error\n",
    "    print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    # Calculating the mean absolute error\n",
    "    print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_regression(X, y):\n",
    "    # Splitting the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "    # Creating the Decision Tree regressor\n",
    "    regressor = DecisionTreeRegressor()\n",
    "\n",
    "    # Fitting the data\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the data\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    # Calculating the r2 score\n",
    "    print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "    # Calculating the mean squared error\n",
    "    print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    # Calculating the mean absolute error\n",
    "    print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def random_forest_regression(X, y):\n",
    "    # Splitting the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "    # Creating the Random Forest Regressor\n",
    "    regressor = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
    "\n",
    "    # Fitting the data\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the data\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    # Calculating the r2 score\n",
    "    print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "    # Calculating the mean squared error\n",
    "    print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    # Calculating the mean absolute error\n",
    "    print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dougl\\Documents\\GitHub\\Comp-Challenge---2023-4\\Code.ipynb Cell 10\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dougl/Documents/GitHub/Comp-Challenge---2023-4/Code.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m regressor \u001b[39m=\u001b[39m KNeighborsRegressor(n_neighbors\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dougl/Documents/GitHub/Comp-Challenge---2023-4/Code.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Fit the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dougl/Documents/GitHub/Comp-Challenge---2023-4/Code.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m regressor\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dougl/Documents/GitHub/Comp-Challenge---2023-4/Code.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dougl/Documents/GitHub/Comp-Challenge---2023-4/Code.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m y_pred \u001b[39m=\u001b[39m regressor\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\dougl\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_regression.py:213\u001b[0m, in \u001b[0;36mKNeighborsRegressor.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the k-nearest neighbors regressor from the training dataset.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[0;32m    196\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39m    The fitted k-nearest neighbors regressor.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39m=\u001b[39m _check_weights(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights)\n\u001b[1;32m--> 213\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y)\n",
      "File \u001b[1;32mc:\\Users\\dougl\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py:400\u001b[0m, in \u001b[0;36mNeighborsBase._fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    399\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(X, (KDTree, BallTree, NeighborsBase)):\n\u001b[1;32m--> 400\u001b[0m         X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, y, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    402\u001b[0m     \u001b[39mif\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    403\u001b[0m         \u001b[39m# Classification targets require a specific format\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[39mif\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\dougl\\anaconda3\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\dougl\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39my cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 964\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    965\u001b[0m     X,\n\u001b[0;32m    966\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m    967\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m    968\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    969\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m    970\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    971\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    972\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m    973\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m    974\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m    975\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m    976\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    977\u001b[0m )\n\u001b[0;32m    979\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric)\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\dougl\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    795\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    797\u001b[0m         )\n\u001b[0;32m    799\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 800\u001b[0m         _assert_all_finite(array, allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    802\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    803\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\dougl\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    108\u001b[0m         allow_nan\n\u001b[0;32m    109\u001b[0m         \u001b[39mand\u001b[39;00m np\u001b[39m.\u001b[39misinf(X)\u001b[39m.\u001b[39many()\n\u001b[0;32m    110\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[0;32m    111\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(X)\u001b[39m.\u001b[39mall()\n\u001b[0;32m    112\u001b[0m     ):\n\u001b[0;32m    113\u001b[0m         type_err \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minfinity\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m allow_nan \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN, infinity\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    115\u001b[0m             msg_err\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    116\u001b[0m                 type_err, msg_dtype \u001b[39mif\u001b[39;00m msg_dtype \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    117\u001b[0m             )\n\u001b[0;32m    118\u001b[0m         )\n\u001b[0;32m    119\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Create a KNN Regression model\n",
    "regressor = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Fit the model\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Print the R2 score\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "    This section analyses the performance of each regression model to determine which technique most accuratley predicts compressive strength\n",
    " \n",
    "This section analyses the performance of each regression model to determine which technique most accuratley predicts compressive strength\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "\n",
    "def test_each_column_with_models(dataset):\n",
    "    # Define the models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Lasso': Lasso(),\n",
    "        'ElasticNet': ElasticNet()\n",
    "    }\n",
    "\n",
    "    # Get the column names\n",
    "    column_names = dataset.columns[:-1]\n",
    "\n",
    "    # Loop over each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Testing {model_name}\")\n",
    "\n",
    "        # Loop over each column\n",
    "        for column in column_names:\n",
    "            # Create the features (X) and target (y)\n",
    "            X = dataset[[column]].values\n",
    "            y = dataset.iloc[:, -1].values\n",
    "\n",
    "            # Split the data into training and testing sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "            # Fit the model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Print the R2 score\n",
    "            print(f\"R2 Score for {column}: {r2_score(y_test, y_pred)}\")\n",
    "\n",
    "            # Print the Mean Squared Error\n",
    "            print(f\"Mean Squared Error for {column}: {mean_squared_error(y_test, y_pred)}\")\n",
    "\n",
    "            # Print the Mean Absolute Error\n",
    "            print(f\"Mean Absolute Error for {column}: {mean_absolute_error(y_test, y_pred)}\")\n",
    "\n",
    "# Call the function\n",
    "test_each_column_with_models(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Predictions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the chosen regression method to predict compresive strngth based on the values given\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
