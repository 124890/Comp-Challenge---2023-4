{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year 2 Computational Challenge - Machine Learning\n",
    "    Douglas Penning, Roman Ogorodnov, Manish Saharan, Ryan Hutchings\n",
    "    \n",
    "***We selected the Random Forest Regressor and found Age to be the most important variable***\n",
    "\n",
    "Overview:\n",
    "\n",
    "We chose to break the programme into key sections, detailed by the headings shown throughout:\n",
    "- Import & Preprocessing\n",
    "\n",
    "    - Data is imported from the .csv file, with the number of duplicate and null entries counted. Removing the **23 duplicate entries** was found to improve the RMSE of the chosen model by **~0.5**. We chose to initially use **simple mean imputer** and **standard scaler** to ensure a consistent and computationally simple approach allowing each model to be tested using the same datasets.\n",
    "        - A seperate file is included showing how we tested various scaler-imputer combinations\n",
    "        - The Compressive Strength (y) is not scaled as this reduces the ability to visually analyse data and performance metrics\n",
    "\n",
    "    - We chose to use an 80-20 train-test split and created the global variables **gX_train_preprocessed, gy_test etc** which are used throughout\n",
    "\n",
    "    - **Note:** While as a group we did discuss the posibility of removing variables initially we felt that it would be a more rigourous approach to iteratively test each dropped combination and then retrain a final model using the new dataset if required. **We have also included code in this cell, which generates a correlation matrix and heatmap (included in our word doc) and found no major multicollinearity between variables**\n",
    "        - We used an arbitrary cut off of 0.8 correlation as little information was available on acceptable limits\n",
    "\n",
    "- Model selection\n",
    "\n",
    "    - We begin by defining each regression function (using a function for each allowed us to quickly individually test & efficiently pass through hyperparameters etc), using the standard parameters in *scikit-learn* and thus having the default best_params = {}. After each regressor is created the global model name dictionary is defined\n",
    "\n",
    "    - Models were tested unoptimised, to reduce computational expense, using 3 error metrics *[3]*:\n",
    "        * **Root Mean Square Error** - Giving differences between actual and predicted values, with the ^2 giving greater weighting to larger errors.\n",
    "        * **R^2** - Measuring correlation between datapoints, used as it is easily visually observed in the plots \n",
    "        * **Mean Absolute Average Error** - While similar to RMSE, MAE is less sensitive to larger outliers\n",
    "    - Models were then ranked on each error type (accounting for the possability for models performing better/worse in specific error types) with **Random Forest** achieving the best score. **Note:** We have tried to ensure this code is resilient, and as such have used best_model and the model dictionary where possible in case a different dataset is better suited to a different model (though this was not possible in the optimisation section).\n",
    "\n",
    "- Optimisation\n",
    "    - We employed a **2 step optimisation process**\n",
    "        1. *Randomised search* - Using a larger grid of parameters to narrow down approximate parameters\n",
    "        2. *Grid search* - Using a smaller grid defined around the results of the randomised search to find the best parameter combination\n",
    "    - We chose a parameter grid based on prior testing as shown in the included files\n",
    "        \n",
    "- Dropping variables\n",
    "    - We chose to drop all possible combinations of variables using an 8 bit binary number as an index, allowing us to analyse any correlation and it's effect on model performance.\n",
    "\n",
    "    - 2 datafiles are produced, showing the binary drop_index and errors, and a second *.xlsx* using the variable names (for single and pairs) allowing easier analysis.\n",
    "    - **As can be seen *AGE* is the most important variable when making predictions**\n",
    "\n",
    "- Graphing & Final data presentation\n",
    "    - Comparitive graphs are shown for each regression model, allowing quick visual comparison \n",
    "    - An interactive line graph, for the chosen model, allows the user to deselect  variables to see the effect on correlation and errors quickly. This uses the gVarErrorDF meaning no extra computation is required\n",
    "    - An interactive bar chart shows the effect removing individual errors has on each error metric, with a baseline shown in red\n",
    "\n",
    "    ***To Run - Please use the Run All option to ensure dataframes are recreated and restart the terminal before re-running***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "This section imports all libraries utilised within the code:\n",
    " - Misc including pandas & numpy etc.\n",
    " - scikit-learn modules\n",
    " - Graphing libraries including matplotlib, as well as ipywidgets and seaborn for interactivity/formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from scipy.stats import norm\n",
    "from typing import Tuple # Used for 'type hinting' in functions\n",
    "\n",
    "# SKLearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, learning_curve\n",
    "\n",
    "\n",
    "# Graphing\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "from ipywidgets import interact, widgets, interactive, fixed\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "# Global Variables defined here to be used throughout the notebook to minimise variable scope issues\n",
    "global gMethodDictionary\n",
    "global gVarErrorDf\n",
    "gHyperparameters = {} # Dictionary to hold the hyperparameters found by randomised search\n",
    "gVarErrorDf = pd.DataFrame(columns=[ 'Drop index','Predicted y', 'RMSE', 'R^2', 'MAE', 'RMSE %', 'R^2 %', 'MAE %'])\n",
    "gVarNames = ['Cement','Blast Furnace Slag','Fly Ash','Water','Superplasticizer','Coarse Aggregate','Fine Aggregate','Age','Concrete compressive strength',' ']\n",
    "gX_train_preprocessed = pd.DataFrame() # Dataframe to hold the preprocessed training data\n",
    "gX_test_preprocessed = pd.DataFrame() # Dataframe to hold the preprocessed testing data\n",
    "gy_train = [] # Series to hold the training data\n",
    "gy_test = [] # Series to hold the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import, handling and preprocessing\n",
    " - Imports, drops duplicated rows, seperates training/target columns, completes basic preprocessing (see overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_import() -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\" \n",
    "    Import the CSV file and return the data as a pandas dataframe\n",
    "\n",
    "    Returns:\n",
    "    X_dataset: (pd.DataFrame): Feature variables (Cement, Blast Furnace Slag, Fly Ash, Water, Superplasticizer, Coarse Aggregate, Fine Aggregate, Age)\n",
    "    y_dataset (pd.Series): Target variable (Concrete compressive strength)\n",
    "    \"\"\"\n",
    "    #import data from the files\n",
    "    dataset = pd.read_csv('Concrete_Data_Yeh_final.csv')\n",
    "\n",
    "    # Format as a dataframe\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    \n",
    "    #print(f'Null values: \\n',dataset.isnull().sum()) #check for null values\n",
    "    print(dataset.duplicated().sum(), 'duplicated rows dropped') # Prints the number of duplicated rows\n",
    "    dataset = dataset.drop_duplicates() # Drop duplicates reducing RMSE by ~0.5\n",
    "    dataset.dtypes #check data types\n",
    "  \n",
    "    # Split the data into X and y\n",
    "    y_dataset = dataset[\"csMPa\"]\n",
    "    X_dataset = dataset.drop(\"csMPa\", axis=1)\n",
    "    \n",
    "    # Creating a correlation matrix to see cross correlation between variables \n",
    "    correlation_matrix = X_dataset.corr()\n",
    "    correlation_matrix.to_excel('correlation_matrix.xlsx')\n",
    "\n",
    "    # Masking diagonal values corresponding to variable correlation with itself\n",
    "    mask = np.ones(correlation_matrix.shape, dtype=bool)\n",
    "    np.fill_diagonal(mask, False)\n",
    "\n",
    "    # Checking for significant correlations in off-diagonal elements\n",
    "    if (abs(correlation_matrix.where(mask)) > 0.8).any().any():\n",
    "        print('One or more variables have significant intercorrelation')\n",
    "    else:\n",
    "        print('No significant intercorrelation between variables')\n",
    "\n",
    "    # Plot the correlation matrix as a heatmap\n",
    "    print(\"\\nCorrelation heatmap:\")    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=plt.cm.Reds)\n",
    "    plt.show()\n",
    "\n",
    "    return X_dataset, y_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_dataset: pd.DataFrame, y_dataset: pd.Series) -> None:\n",
    "    \"\"\" \n",
    "    Preprocess the data (simple imputer (mean), standard scaler) and split into training and test sets\n",
    "\n",
    "    Parameters:\n",
    "    X_dataset (pd.DataFrame): The feature variables\n",
    "    y_dataset (pd.Series): The target variable (concrete compressive strength)\n",
    "    \n",
    "    Returns: None (global variables are set)\n",
    "    \"\"\"\n",
    "    # Splitting the data into training and test sets\n",
    "    global gX_train_preprocessed, gX_test_preprocessed\n",
    "    global gy_train, gy_test    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Creating a preprocessing pipeline that imputes missing values with the mean and scales the data\n",
    "    preprocessing_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    # Setting the global variables\n",
    "    gX_train_preprocessed = pd.DataFrame(preprocessing_pipeline.fit_transform(X_train), columns=X_train.columns)\n",
    "    gX_test_preprocessed = pd.DataFrame(preprocessing_pipeline.transform(X_test), columns=X_test.columns)\n",
    "    gy_train = y_train\n",
    "    gy_test = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression functions\n",
    "\n",
    "This section defines functions for various regression models which are later compared to choose the most effective model.\n",
    "\n",
    "The regression models included are:\n",
    "\n",
    "1. **Linear Regression (`linear_regression`)**: While it is stated that data is nonlinear we used it as a baseline.\n",
    "\n",
    "2. **Decision Tree Regression (`decision_tree_regression`)**\n",
    "\n",
    "3. **Random Forest Regression (`random_forest_regression`)**: **Chosen model**\n",
    "\n",
    "4. **Lasso Regression (`lasso_regression`)**\n",
    "\n",
    "5. **Elastic Net Regression (`elastic_net_regression`)**\n",
    "\n",
    "6. **Ridge Regression (`ridge_regression`)**\n",
    "\n",
    "7. **Support Vector Regression (`svr_regression`)**\n",
    "\n",
    "8. **K-Nearest Neighbors Regression (`knn_regression`)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(xTrainData: pd.DataFrame, yTrainData: pd.Series, yTestData: pd.Series, bestFit = {}) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function creates and fits a linear regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (pandas.Series): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    y_pred (numpy.ndarray): The predicted values of compressive strength for the test set\n",
    "    \"\"\" \n",
    "   \n",
    "    regressor = LinearRegression(**bestFit) # Creating the Linear Regression model\n",
    "    regressor.fit(xTrainData, yTrainData) # Fitting the data\n",
    "    y_pred = regressor.predict(yTestData) # Making predictions\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_regression(xTrainData: pd.DataFrame, yTrainData: pd.Series, yTestData: pd.Series, bestFit = {}) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function creates and fits a decision tree regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (pandas.Series): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    y_pred (numpy.ndarray): The predicted values of compressive strength for the test set\n",
    "    \"\"\" \n",
    "\n",
    "    regressor = DecisionTreeRegressor(**bestFit) # Creating the Decision Tree regressor\n",
    "    regressor.fit(xTrainData, yTrainData)\n",
    "    y_pred = regressor.predict(yTestData)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_regression(xTrainData: pd.DataFrame, yTrainData: pd.Series, yTestData: pd.Series, bestFit = {})-> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function creates and fits a random forest regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (pandas.Series): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    y_pred (numpy.ndarray): The predicted values of compressive strength for the test set\n",
    "    \"\"\" \n",
    "   \n",
    "    regressor = RandomForestRegressor(**bestFit) # Creating the Random Forest Regressor\n",
    "    regressor.fit(xTrainData, yTrainData) \n",
    "    y_pred = regressor.predict(yTestData)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lasso(xTrainData: pd.DataFrame, yTrainData: pd.Series, yTestData: pd.Series, bestFit = {}) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function creates and fits a Lasso regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (pandas.Series): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    y_pred (numpy.ndarray): The predicted values of compressive strength for the test set\n",
    "    \"\"\" \n",
    "    \n",
    "    model = Lasso(**bestFit) # Create and train the model\n",
    "    model.fit(xTrainData, yTrainData)\n",
    "    y_pred = model.predict(yTestData)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_net_regression(xTrainData: pd.DataFrame, yTrainData: pd.Series, yTestData: pd.Series, bestFit = {}) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function creates and fits a elastic net regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (pandas.Series): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    y_pred (numpy.ndarray): The predicted values of compressive strength for the test set\n",
    "    \"\"\" \n",
    "\n",
    "    regressor = ElasticNet(**bestFit)    # Creating the Elastic Net Regressor\n",
    "    regressor.fit(xTrainData, yTrainData)\n",
    "    y_pred = regressor.predict(yTestData)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(xTrainData: pd.DataFrame, yTrainData: pd.Series, yTestData: pd.Series, bestFit = {}) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function creates and fits a ridge regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (pandas.Series): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    y_pred (numpy.ndarray): The predicted values of compressive strength for the test set\n",
    "    \"\"\" \n",
    "\n",
    "    regressor = Ridge(**bestFit) # Creating the Ridge Regressor\n",
    "    regressor.fit(xTrainData, yTrainData)\n",
    "    y_pred = regressor.predict(yTestData)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_svr(xTrainData: pd.DataFrame, yTrainData: pd.Series, yTestData: pd.Series, bestFit = {}) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function creates and fits a support vector regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (pandas.Series): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    y_pred (numpy.ndarray): The predicted values of compressive strength for the test set\n",
    "    \"\"\" \n",
    "   \n",
    "    model = SVR(**bestFit)  # Create and train the model\n",
    "    model.fit(xTrainData, yTrainData)\n",
    "    y_pred = model.predict(yTestData)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_knn(xTrainData: pd.DataFrame, yTrainData: pd.Series, yTestData: pd.Series, bestFit = {})-> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function creates and fits a KNN regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (pandas.Series): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    y_pred (numpy.ndarray): The predicted values of compressive strength for the test set\n",
    "    \"\"\" \n",
    "\n",
    "    model = KNeighborsRegressor(**bestFit)    # Create and train the model\n",
    "    model.fit(xTrainData, yTrainData)\n",
    "    y_pred = model.predict(yTestData)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a dictionary containing Name:Function pairs for each regression method\n",
    "gMethodDictionary = [('Linear Regression', linear_regression),\n",
    "                    ('Decision Tree Regression', decision_tree_regression),\n",
    "                    ('Random Forest Regression', random_forest_regression),\n",
    "                    ('Ridge', ridge_regression), \n",
    "                    ('Lasso', perform_lasso),\n",
    "                    ('Elastic Net', elastic_net_regression),\n",
    "                    ('Support Vector Regression', perform_svr),\n",
    "                    ('K-Nearest Neighbors Regression', perform_knn)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "- Analysing the performance of each regression model to determine which technique most accuratley predicts compressive strength\n",
    "- Uses metrics:\n",
    "    1) Root Mean Square Error\n",
    "    2) R^2\n",
    "    3) Mean Absolute Error\n",
    "- Models are ranked on **ALL** metrics to find the best model (though it was observed ranking was consistent across all metrics at this stage)\n",
    "- Dataframe exported to Excel for review\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_performance() -> Tuple[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    This function evaluates the performance of all regression models defined, iterating through the global `gMethodDictionary`.\n",
    "    Predicted values are used to calculate the root mean squared error (RMSE), R^2 score, and mean absolute error (MAE).\n",
    "    The results are stored in a DataFrame.\n",
    "\n",
    "    Models are ranked based on each performance metric, with the highest R^2 score and the lowest \n",
    "    RMSE and MAE being the best. \"rank sum\" is calculated for each model with the model with the lowest selected.\n",
    "\n",
    "    Parameters: None\n",
    "\n",
    "    Returns:\n",
    "    best_model (str): The name of the best model.\n",
    "    df (pd.DataFrame): A DataFrame containing the performance metrics and ranks for each model.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    for model_name, model in gMethodDictionary: # Loop over each model\n",
    "        # Calling each regression model, passing in the training and test data (Note: models are unoptimised so bestFit assumes default parameters))      \n",
    "        y_pred = model(gX_train_preprocessed, gy_train, gX_test_preprocessed)\n",
    "\n",
    "        # Calculate the performance metrics\n",
    "        rmse = sqrt(mean_squared_error(gy_test, y_pred))\n",
    "        r2 = r2_score(gy_test, y_pred)\n",
    "        mae = mean_absolute_error(gy_test, y_pred)\n",
    "    \n",
    "        # Append the data to the list\n",
    "        data.append([model_name, y_pred, rmse, r2, mae])\n",
    "\n",
    "    # Create a DataFrame from the data\n",
    "    df = pd.DataFrame(data, columns=['Model', 'y_pred', 'RMSE', 'R^2','MAE'])\n",
    "\n",
    "    # Set the model names as the index\n",
    "    df.set_index('Model', inplace=True)\n",
    "\n",
    "    # Rank each metric, with the highest being the best for R^2 and the lowest being the best for RMSE and MAE\n",
    "    df['R^2_rank'] = df['R^2'].rank(ascending=False)\n",
    "    df['RMSE_rank'] = df['RMSE'].rank()\n",
    "    df['MAE_rank'] = df['MAE'].rank()\n",
    "\n",
    "    \n",
    "    df['rank_sum'] = df['R^2_rank'] + df['RMSE_rank'] + df['MAE_rank'] # Calculate the sum of the ranks\n",
    "\n",
    "    df.to_excel('regression_performance.xlsx') # Exporting the dataframe to excel\n",
    "\n",
    "    # Find the model with the lowest rank sum\n",
    "    best_model = df['rank_sum'].idxmin()\n",
    "\n",
    "    return best_model, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "As shown Random Forest Regression has the best untuned performance on our dataset and as such we are only analysing hyperparameters for this model:\n",
    "The most important hyperparameters to tune in a Random Forest model to improve its performance are:\n",
    "\n",
    "1. `n_estimators`: Higher number of trees give you better performance but is more computationally intense.\n",
    "\n",
    "2. `max_depth`: Can help to prevent overfitting. If the max depth is too high, the model may learn too much from the training data and perform poorly on test data.\n",
    "\n",
    "3. `min_samples_split`: Increasing this parameter increases the number of samples considered at each node.\n",
    "\n",
    "4. `min_samples_leaf`: Similar to min_samples_splits, instead describing the minimum number of samples of samples at the leafs\n",
    "\n",
    "5. `max_features`: The number of features to consider when looking for the best split. When using \"auto\", `max_features=sqrt(n_features)`.\n",
    "\n",
    "6. `bootstrap`: If False, the whole dataset is used to build each tree. We chose to only use False throught as preforming initial testing found true to yield significantly lower performance\n",
    "\n",
    "We chose to test these along with minimum impurity decrease.\n",
    "\n",
    "**Random Search** sets up a grid of hyperparameter values and selects random combinations and is therefore less computationally expensive then Grid Search (and was found to be better optimised for multi-core processing). We use this step first, as while efficient it's not guaranteed to find the best parameters.\n",
    "\n",
    "**Grid Search** systematically works through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. We use a smaller grid defined around the results of the random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_grid_search()-> dict:\n",
    "    \"\"\"\n",
    "    This function performs a randomised search to narrow down the hyperparameter grid for the Random Forest Regressor.\n",
    "    It then performs a grid search based on the randomised search results, to find the best hyperparameter combination.\n",
    "\n",
    "    Parameters: None\n",
    "    Returns: best_grid_params (dict): The best hyperparameter combination\n",
    "    \"\"\"\n",
    "    ### Included parameters were selected based on the results of the of several larger tests, one of which is seen in the appendix of the report and Code_hyperparameters\n",
    "    ### We made use of several large grid search runs lasting over 12 hours and were able to achieve the best results with parameters covered in the grids below\n",
    "    ### {'bootstrap': False, 'max_depth': 25, 'max_features': 'sqrt', 'min_samples_split': 3, 'n_estimators': 160} - Gave the best results observed\n",
    "    ### RMSE = 3.986254, R^2 = 0.931, MAE = 2.975 before removing damaging variables\n",
    "\n",
    "    ### NOTE: To decrease runtime, the number of iterations for the randomised search has been reduced to 450 potentially harming the results.\n",
    "\n",
    "\n",
    "    # Define a hyperparameter grid for randomised search\n",
    "    param_random = {\n",
    "        'n_estimators': [50, 100, 125, 135, 145, 155, 170], # 7\n",
    "        'max_depth': [None, 20, 30, 40], #4\n",
    "        'min_samples_split': [ 2, 3, 5, 10], #4\n",
    "        'min_samples_leaf': [ 1, 2, 5, 10],#4\n",
    "        'min_impurity_decrease': [0.0, 0.2, 0.4, 0.5],#4\n",
    "        'bootstrap': [False], #1\n",
    "        'max_features': ['auto', 'sqrt'] #2\n",
    "    }\n",
    "\n",
    "    # Perform randomised search over the larger parameter grid (verbose specifies how much information is displayed)\n",
    "    rf_random = RandomizedSearchCV(estimator=RandomForestRegressor(), param_distributions=param_random, n_iter=450, cv=5, n_jobs=-1, verbose=0)\n",
    "    rf_random.fit(gX_train_preprocessed, gy_train)\n",
    "\n",
    "    # Identify the best hyperparameter combination from random search\n",
    "    best_random_params = rf_random.best_params_\n",
    "\n",
    "    # Define a narrower hyperparameter grid around the best hyperparameter combination (if statements used to ensure values are not out of range)\n",
    "    param_grid = {\n",
    "        'n_estimators': [best_random_params['n_estimators'] - 10, best_random_params['n_estimators'], best_random_params['n_estimators'] + 10],\n",
    "        'min_samples_split': [best_random_params['min_samples_split'] - 1 if best_random_params['min_samples_split'] > 2 else 2, best_random_params['min_samples_split'], best_random_params['min_samples_split'] + 1],\n",
    "        'min_samples_leaf': [best_random_params['min_samples_leaf'] - 1 if best_random_params['min_samples_leaf'] > 1 else 2, best_random_params['min_samples_leaf'], best_random_params['min_samples_leaf'] + 1],\n",
    "        'min_impurity_decrease': [best_random_params['min_impurity_decrease'] - 0.1 if best_random_params['min_impurity_decrease'] > 0.1 else 0, best_random_params['min_impurity_decrease'], best_random_params['min_impurity_decrease'] + 0.1],\n",
    "        'bootstrap': [best_random_params['bootstrap']],\n",
    "        'max_features': [best_random_params['max_features']]\n",
    "    }\n",
    "    # If the best max_depth is None, then only test None, otherwise test the best value +/- 5 \n",
    "    if best_random_params['max_depth'] == None:\n",
    "        param_grid['max_depth'] = [None]\n",
    "    else:\n",
    "        param_grid['max_depth'] = [best_random_params['max_depth'] - 5, best_random_params['max_depth'], best_random_params['max_depth'] + 5]\n",
    "\n",
    "    global gHyperparameters \n",
    "    gHyperparameters = param_grid # Setting the global variable to be used in the final run (testing hyperameter combination on the new dataset)\n",
    "\n",
    "    # Perform grid search\n",
    "    rf_grid = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=5, n_jobs=-1, verbose=0)\n",
    "    rf_grid.fit(gX_train_preprocessed, gy_train)\n",
    "\n",
    "    # Identify the best hyperparameter combination from grid search\n",
    "    best_grid_params = rf_grid.best_params_\n",
    "\n",
    "    # Use the best hyperparameter combination to train the final Random Forest model\n",
    "    final_rf_model = RandomForestRegressor(**best_grid_params)\n",
    "    final_rf_model.fit(gX_train_preprocessed, gy_train)\n",
    "\n",
    "    return best_grid_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_unopt_random_forest(best_grid_params: dict) -> dict:\n",
    "    \"\"\"\n",
    "    This function compares the performance of the original unoptimised Random Forest Regressor model with the optimised Random Forest Regressor model.\n",
    "    The model trained, using the hyperparameter combination identified by the grid search, performance metrics are compared to the original model. If the\n",
    "    optimised model performs worse than the original model, the grid search is deemed unsuccessful and an empty dictionary is returned.\n",
    "\n",
    "    Parameters: \n",
    "    best_grid_params (dict): The best hyperparameter combination from the grid search.\n",
    "    \n",
    "    Returns:\n",
    "    best_grid_params (dict): The best hyperparameter combination in case the optimisation was unsuccessful.\n",
    "    \"\"\" \n",
    "    # Original unoptimised Random Forest Regression\n",
    "    y_pred_unoptimised = random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed)\n",
    "\n",
    "    # Optimized Random Forest Regression  \n",
    "    y_pred_optimised = random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed,best_grid_params)\n",
    "\n",
    "    # Calculate errors\n",
    "    rmse_unoptimised = sqrt(mean_squared_error(gy_test, y_pred_unoptimised))\n",
    "    rmse_optimised = sqrt(mean_squared_error(gy_test, y_pred_optimised))\n",
    "    r2_unoptimised = r2_score(gy_test, y_pred_unoptimised)\n",
    "    r2_optimised = r2_score(gy_test, y_pred_optimised)\n",
    "    mae_unoptimised = mean_absolute_error(gy_test, y_pred_unoptimised)\n",
    "    mae_optimised = mean_absolute_error(gy_test, y_pred_optimised)\n",
    "\n",
    "    # Print errors\n",
    "    print(\"Unoptimised RMSE:\", rmse_unoptimised)\n",
    "    print(\"Optimised RMSE:\", rmse_optimised)\n",
    "    print(\"Unoptimised R^2:\", r2_unoptimised)\n",
    "    print(\"Optimised R^2:\", r2_optimised)\n",
    "    print(\"Unoptimised MAE:\", mae_unoptimised)\n",
    "    print(\"Optimised MAE:\", mae_optimised)\n",
    "\n",
    "    # If the optimised model performs worse than the original model, the grid search was unsuccessful. An empty dictionary is returned.\n",
    "    if rmse_optimised > rmse_unoptimised:\n",
    "        print(\"Optimisation unsuccessful\")\n",
    "        best_grid_params = {}\n",
    "    return best_grid_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping variables\n",
    "\n",
    "    var_drop():\n",
    "Used to create a new data set, dropping all combinations of variables using a binary 'drop index'. This allows us to determine the effect on regression performance when dropping single, pairs, triplets or any other combination of variables. This was chosen to allow us to clearly see and account for variable correlation, and efficiently plot the interactive graphs as shown.\n",
    "\n",
    "    var_performance():\n",
    "Taking the dataset from var_drop(): to measure the performance effect of dropping variables\n",
    "\n",
    "    var_impact_pct():\n",
    "Calculating the difference in original vs retrained performance for various combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_drop(dropVar: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Drops columns from the training and testing datasets based on a binary number.\n",
    "    Each digit in the binary number corresponds to a column in the datasets. If the digit is 1, the corresponding column is dropped.\n",
    "    If the digit is 0, the corresponding column is kept.\n",
    "\n",
    "    Parameters:\n",
    "    dropVar (int): An intiger, converted to binary with each digit representing a column in the datasets.\n",
    "\n",
    "    Returns:\n",
    "    datasetTrain_drop (pd.DataFrame): The training dataset with the columns dropped.\n",
    "    datasetTest_drop (pd.DataFrame): The testing dataset with the columns dropped.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the binary number to a list of booleans\n",
    "    bool_list = [bool(int(x)) for x in format(dropVar, '0{}b'.format(len(gX_train_preprocessed.columns)))]\n",
    "\n",
    "    # Create a list of column names to drop based on the boolean list\n",
    "    columns_to_drop_train = gX_train_preprocessed.columns[bool_list]\n",
    "    columns_to_drop_test = gX_test_preprocessed.columns[bool_list]\n",
    "\n",
    "    # Drop the columns\n",
    "    datasetTrain_drop = gX_train_preprocessed.drop(columns_to_drop_train, axis=1)\n",
    "    datasetTest_drop = gX_test_preprocessed.drop(columns_to_drop_test, axis=1)\n",
    "\n",
    "    return datasetTrain_drop, datasetTest_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_performance(best_grid_params: dict, best_model: str) -> None:\n",
    "    \"\"\"\n",
    "    This function evaluates the performance of the best regression model for every combination of dropped variables.\n",
    "    best_model is used to identify the best regression model (ensuring resiliance to changes in the gMethodDictionary)\n",
    "\n",
    "    It then loops over all variables in the preprocessed training and testing data, dropping one variable at a time \n",
    "    and evaluating the performance of the model without that variable. \n",
    "\n",
    "    The function also calculates the variation of each performance metric compared to the base case (i.e., when no \n",
    "    variables are dropped). The results are stored in the global DataFrame `gVarErrorDf`.\n",
    "\n",
    "    Parameters:\n",
    "    best_grid_params (dict): The best hyperparameter combination as found by the optimisation function.\n",
    "    best_model (str): The name of the best regression model.\n",
    "    \n",
    "    Returns:    None\n",
    "    \"\"\"\n",
    "    # Copy the preprocessed training and testing data\n",
    "    x_train_drop = gX_train_preprocessed.copy()\n",
    "    x_test_drop = gX_test_preprocessed.copy()\n",
    "    \n",
    "    # Find the function corresponding to the best model\n",
    "    for key, model_function in gMethodDictionary:\n",
    "        if key == best_model:\n",
    "            break\n",
    "    \n",
    "    # Loop over all possible combinations of dropped variables\n",
    "    for i in range(255):\n",
    "        x_train_drop, x_test_drop = var_drop(i)\n",
    "       \n",
    "       # Calling the random forest regression model training using the dropped variable data\n",
    "        y_pred = model_function(x_train_drop, gy_train, x_test_drop, best_grid_params)\n",
    "\n",
    "        # Calculate the performance metrics\n",
    "        rms_error = sqrt(mean_squared_error(gy_test, y_pred))\n",
    "        r2_error = r2_score(gy_test, y_pred)\n",
    "        ma_error = mean_absolute_error(gy_test, y_pred)\n",
    "        if i == 0:\n",
    "            rms_var = 1\n",
    "            r2_var = 1\n",
    "            ma_var = 1\n",
    "            rms_base = rms_error\n",
    "            r2_base = r2_error\n",
    "            ma_base = ma_error\n",
    "\n",
    "        else:\n",
    "            rms_var = rms_error / rms_base\n",
    "            r2_var = r2_error / r2_base\n",
    "            ma_var = ma_error / ma_base\n",
    "\n",
    "\n",
    "        gVarErrorDf.loc[i] = [bin(i), y_pred, rms_error, r2_error, ma_error, rms_var, r2_var, ma_var] # Adding the results to the dataframe\n",
    "    gVarErrorDf.to_excel('var_performance.xlsx') # Exporting the dataframe to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_impact_pct() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function calculates the impact of each variable on the model's performance metrics (RMSE, R^2, and MAE) \n",
    "    when the variable is dropped. A DataFrame is created containing performance metrics each singl/ double variable\n",
    "    drop. \n",
    "\n",
    "    The function then ranks each model  based on the performance metrics, \n",
    "    calculates a rank sum for each model, and sorts the DataFrame by the rank sum. The model with the lowest rank \n",
    "    sum is considered the best model.\n",
    "    The sorted DataFrame is then exported to an excel file.\n",
    "\n",
    "    Parameters: None\n",
    "\n",
    "    Returns: impact_df (pd.DataFrame): A DataFrame containing the performance metrics for each model.\n",
    "    \"\"\"\n",
    "\n",
    "    impact_df = pd.DataFrame(columns=['Dropped variable: 1','Dropped variable : 2', 'RMSE', 'R^2', 'MAE','RMSE %', 'R^2 %', 'MAE %', ])\n",
    "\n",
    "    # NOTE: Single variable impact\n",
    "    for i in range(8):\n",
    "        index = 2**i\n",
    "        impact_df.loc[i] = [gVarNames[(7-i)],gVarNames[9], gVarErrorDf.loc[index]['RMSE'], gVarErrorDf.loc[index]['R^2'], gVarErrorDf.loc[index]['MAE'], gVarErrorDf.loc[index]['RMSE %'], gVarErrorDf.loc[index]['R^2 %'], gVarErrorDf.loc[index]['MAE %']]\n",
    "    \n",
    "    #Data frame containing all single variable impacts\n",
    "    impact_df.loc[8] = [\"None dropped\",'', gVarErrorDf.loc[0]['RMSE'], gVarErrorDf.loc[0]['R^2'], gVarErrorDf.loc[0]['MAE'], gVarErrorDf.loc[0]['RMSE %'], gVarErrorDf.loc[0]['R^2 %'], gVarErrorDf.loc[0]['MAE %']] \n",
    "    \n",
    "    # NOTE: Double variable impact\n",
    "    l = 9\n",
    "    for j in range(7):\n",
    "        #loop from j+1 to 8\n",
    "        for k in range(j+1,8):\n",
    "            if j != k:\n",
    "                index = 2**j + 2**k #7- index to move through gVarNames correctly\n",
    "                # Data frame containing all double variable impacts\n",
    "                impact_df.loc[l] = [gVarNames[(7-j)], gVarNames[(7-k)], gVarErrorDf.loc[index]['RMSE'], gVarErrorDf.loc[index]['R^2'], gVarErrorDf.loc[index]['MAE'], gVarErrorDf.loc[index]['RMSE %'], gVarErrorDf.loc[index]['R^2 %'], gVarErrorDf.loc[index]['MAE %']]\n",
    "                l += 1\n",
    "\n",
    "    # Rank each metric, with the highest being the best for R^2 and the lowest being the best for RMSE and MAE\n",
    "    impact_df['R^2_rank'] = impact_df['R^2'].rank(ascending=False)\n",
    "    impact_df['RMSE_rank'] = impact_df['RMSE'].rank()\n",
    "    impact_df['MAE_rank'] = impact_df['MAE'].rank()\n",
    "\n",
    "    # Calculate the sum of the ranks\n",
    "    impact_df['rank_sum'] = impact_df['R^2_rank'] + impact_df['RMSE_rank'] + impact_df['MAE_rank']\n",
    "\n",
    "    # Find the model with the lowest rank sum\n",
    "    best_model = impact_df['rank_sum'].idxmin()\n",
    "    \n",
    "    impact_df.sort_values(by=['rank_sum'], inplace=True) # Sort the data frame based on the rank sum\n",
    "    impact_df.to_excel('variable_drop_impact.xlsx')#export to excel\n",
    "    \n",
    "    return impact_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Predictions\n",
    "    \n",
    "Using all information gathered to create a retrained model (found to be Random Forest Regression) removing variables found to harm performance. Hyperparameters are re-optimised (based on the previous results from the randomised search) for this new data set. Metrics and results are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_drop_final()-> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Drops columns from the training and testing datasets based on the binary number with the lowest RMSE.\n",
    "    The function first sorts the global DataFrame `gVarErrorDf`, to find the drop index with the lowest errors.\n",
    "    The function then creates copies of the global DataFrames `gX_train_preprocessed` and `gX_test_preprocessed`, and drops the specified columns from these copies.\n",
    "\n",
    "    Parameters: None\n",
    "\n",
    "    Returns:\n",
    "    X_train_final (pd.DataFrame): The training dataset with the columns dropped.\n",
    "    X_test_final (pd.DataFrame): The testing dataset with the columns dropped.\n",
    "    \"\"\"\n",
    "    #sort gVarError by RMSE to determine which index to use\n",
    "    varError_sort = gVarErrorDf.sort_values(by=['RMSE'], ascending = True,  inplace=False)\n",
    "    drop_string = varError_sort['Drop index'].iloc[0]\n",
    "\n",
    "    # Convert the binary number to a list of booleans\n",
    "    bool_list = [bool(int(x)) for x in drop_string[2:]]\n",
    "    #bool list needs to be made up to 8 digits\n",
    "    while len(bool_list) < 8:\n",
    "        bool_list.insert(0, False)\n",
    "    \n",
    "    # Creates a copy of gX_train_preprocessed etc and drops the columns\n",
    "    X_train_final = gX_train_preprocessed.copy()\n",
    "    X_test_final = gX_test_preprocessed.copy()\n",
    "    columns_to_drop = gX_train_preprocessed.columns[bool_list] # Train and test data have the same columns\n",
    "\n",
    "    #print out the names of the columns to drop\n",
    "    print('Dropped variables:', columns_to_drop.tolist())\n",
    "\n",
    "    # Drop the columns\n",
    "    X_train_final.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    X_test_final.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    return X_train_final, X_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_hyperparameters(finalX_train: pd.DataFrame, finalX_test: pd.DataFrame) -> Tuple[dict, pd.Series]:\n",
    "    \"\"\"\n",
    "    Rerunning hyperparameter optimisation as dropping variables may change the best hyperparameter combination.\n",
    "\n",
    "    Parameters:\n",
    "    finalX_train (pd.DataFrame): The training dataset with the columns dropped.\n",
    "    finalX_test (pd.DataFrame): The testing dataset with the columns dropped.\n",
    "\n",
    "    Returns:\n",
    "    final_grid_params (dict): The best hyperparameter combination.\n",
    "    y_pred_final (pd.Series): The predicted values of compressive strength for the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrain hyperparameters\n",
    "    param_grid = gHyperparameters # Using the global variable set based on the randomised search carried out in random_grid_search()\n",
    "\n",
    "    # Perform grid search\n",
    "    rf_grid = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=5, verbose = 0)\n",
    "    rf_grid.fit(finalX_train, gy_train)\n",
    "\n",
    "    final_grid_params = rf_grid.best_params_ # Find the best hyperparameter combination from grid search\n",
    "\n",
    "    #Final model\n",
    "    y_pred_final = random_forest_regression(finalX_train, gy_train, finalX_test, final_grid_params)\n",
    "\n",
    "    # Calculate errors\n",
    "    rmse_final = sqrt(mean_squared_error(gy_test, y_pred_final))\n",
    "    r2_final = r2_score(gy_test, y_pred_final)\n",
    "    mae_final = mean_absolute_error(gy_test, y_pred_final)\n",
    "\n",
    "    # Print errors\n",
    "    print(\"Final RMSE:\", rmse_final)\n",
    "    print(\"Final R^2:\", r2_final)\n",
    "    print(\"Final MAE:\", mae_final)\n",
    "\n",
    "    return final_grid_params, y_pred_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_error_analysis( y_pred_final: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Calculates the mean error, standard deviation of errors, and 95% confidence interval of errors in the final model's predictions.\n",
    "    Metrics are printed.\n",
    "\n",
    "    Parameters:\n",
    "    y_pred_final (np.ndarray): The predicted values from the final model.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculates uncertainties in the final model's predictions\n",
    "    prediction_errors = gy_test - y_pred_final\n",
    "    mean_error = np.mean(abs(prediction_errors))\n",
    "    std_dev_error = np.std(prediction_errors)\n",
    "    conf_interval_error = np.percentile(prediction_errors, [2.5, 97.5])\n",
    "\n",
    "    print(f\"Mean absolute error: {round(mean_error,5)} CsMPa (5 decimal places)\")\n",
    "    print(f\"Standard deviation of errors: {round(std_dev_error,5)} CsMPa (5 decimal places)\")\n",
    "    print(f\"95% confidence interval of errors: {conf_interval_error} CsMPa\")\n",
    "\n",
    "    # calculates the percentage of predictions that are within 10% of the actual values & 25%\n",
    "    within_10_percent = np.abs(prediction_errors) < 0.1 * gy_test\n",
    "    percentage_within_10 = np.mean(within_10_percent)\n",
    "\n",
    "    print(f\"Percentage of predictions that are within 10% of the actual values: {round(percentage_within_10*100,2)} %\")\n",
    "\n",
    "    within_25_percent = np.abs(prediction_errors) < 0.25 * gy_test\n",
    "    percentage_within_25 = np.mean(within_25_percent)\n",
    "\n",
    "    print(f\"Percentage of predictions that are within 25% of the actual values: {round(percentage_within_25*100,2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Functions\n",
    "\n",
    "1. `comp_graph()` - *Static*\n",
    "\n",
    "    This function creates subplots allowing visual comparison between all regression functions.\n",
    "\n",
    "2. `var_effect_line()` - *Interactive*\n",
    "\n",
    "    This function is used to visualise the effect of a variable on the outcome. The user can choose any combination of variables to drop by deselecting a checkbox, the graph shows predicted vs actual values of Compressive strength, and the error metrics are shown rounded to 4 decimal places.\n",
    "\n",
    "3. `var_effect_bar()` - *Interactive*\n",
    "\n",
    "    This function is used to create a bar chart showing the effect on the various errors, of removing individual variables from the dataset. Users can choose the error type from the drop down menu.\n",
    "\n",
    "4. `plot_learning_curve()` - *Static*\n",
    "\n",
    "    This graph is run as part of our *Final_run()* and shows the learning curves for the final optimised model, using a dataset dropping variables found to harm performance compared to the unoptimised Ranom Forrest Model. It shows how the amount of training data effects model errors.\n",
    "\n",
    "5. `error_histogram()` - *Interactive*\n",
    "\n",
    "    This interactive histogram shows the error distribution for the final model (approximating a normal distribution) providing crucial infromation when analysing the reliability of predictions. Bar width can be altered by the user with the slider.\n",
    "\n",
    "6. `visualize_rf_trees` - *Static*\n",
    "\n",
    "    **Not run** but included to show the growth of the \"random forest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_graph() -> None:\n",
    "    \"\"\"\n",
    "    This function generates a comparative graph of actual vs predicted values for different regression models.\n",
    "\n",
    "    The function calls the `regression_performance` function to get the best model and a DataFrame containing \n",
    "    the predictions of each model. It then creates a scatter plot for each model, comparing the actual and \n",
    "    predicted values. The scatter plots are arranged in a grid, with one subplot for each model with a y=x line added.\n",
    "\n",
    "    The function doesn't return anything. The graph is displayed using `plt.show()`.\n",
    "\n",
    "    Note: This function relies on the global variable `gy_test` for the actual values. Make sure this variable \n",
    "    is defined and properly initialized before calling this function.\n",
    "\n",
    "    Parameters: None\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Number of models taking the data from the data frame in the regression performance function\n",
    "    best_model, df = regression_performance()\n",
    "    n_models = len(df)\n",
    "\n",
    "    # Create a figure and axes with a subplot for each model\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(20, 15))\n",
    "\n",
    "    # Flatten the axes array\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Loop over each model\n",
    "    for i, (model_name, row) in enumerate(df.iterrows()):         # Plot y_test vs y_pred\n",
    "        axs[i].scatter(gy_test, row['y_pred'], s=10)\n",
    "        axs[i].plot([gy_test.min(), gy_test.max()], [gy_test.min(), gy_test.max()], 'k--', lw=2)\n",
    "        axs[i].set_xlabel('Actual CsMPa', fontsize=12)\n",
    "        axs[i].set_ylabel('Predicted CsMPa', fontsize=12)\n",
    "        axs[i].set_title(f'{model_name}')\n",
    "        axs[i].xaxis.set_minor_locator(AutoMinorLocator())\n",
    "        axs[i].yaxis.set_minor_locator(AutoMinorLocator())\n",
    "        axs[i].grid(False)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for i in range(n_models, len(axs)):\n",
    "        fig.delaxes(axs[i])\n",
    "\n",
    "    # Add overall figure title\n",
    "    fig.suptitle('Comparative Graph of Actual vs Predicted Values of Compressive Strength (CsMPa) \\n (y=x line added for reference) \\n', fontsize=15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_effect_line() -> None:\n",
    "    \"\"\" \n",
    "    This function creates an interactive scatter plot of actual vs predicted values for the best model, with a trendline.\n",
    "    Checkboxes are added to the plot, allowing the user to remove variables from the model. The plot is updated when the user clicks a checkbox.\n",
    "    A text box is added to the plot, containing the effect of removing each variable on the error metrics.\n",
    "\n",
    "    Parameters: None\n",
    "    Returns: None (The plot is displayed using `plt.show()`)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of checkboxes, taking the variable names from the global variable `gVarNames`\n",
    "    checkboxes = [widgets.Checkbox(value=True, description=var) for var in gVarNames[0:8]]\n",
    "\n",
    "    def update_lines(*args, **kwargs):\n",
    "        \"\"\"\n",
    "        This function updates the scatter plot when the user clicks a checkbox.\n",
    "\n",
    "        Parameters:\n",
    "        *args: A list of arguments. The first argument is a boolean value indicating whether the checkbox is checked.\n",
    "        **kwargs: A dictionary of keyword arguments. The key is the name of the checkbox, and the value is a boolean indicating whether the checkbox is checked.\n",
    "\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        active_vars = [var.description for var in checkboxes if var.value] # Get the names of the active variables\n",
    "        # Checks that at least one checkbox (variable) is selected, if not an error is printed and the function returns\n",
    "        if not active_vars:\n",
    "            print(\"Error: At least one checkbox must be selected.\")\n",
    "            return\n",
    "        \n",
    "        clear_output(wait=True) # Clear the output of the cell\n",
    "        #sns.set_style(\"whitegrid\") # Set the style of the plot\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6)) # Create a figure and axes\n",
    "        plt.size=(10, 6)\n",
    "\n",
    "        fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)  # Center the plot\n",
    "        active_vars = [var.description for var in checkboxes if var.value]\n",
    "        binary_string = ''.join(['1' if not checkbox.value else '0' for checkbox in checkboxes]) # Create a binary string representing the active variables\n",
    "        print(f'Drop Index: {binary_string}')\n",
    "        binary_int = int(binary_string,2)\n",
    "\n",
    "        x = gy_test\n",
    "        y = gVarErrorDf.loc[(binary_int)].iloc[1] # Using the binary string to select the correct row in the global DataFrame `gVarErrorDf`\n",
    "        ax.scatter(x, y, label='Data')  # Add a label to the scatter plot\n",
    "        \n",
    "        # Fit a line to the data\n",
    "        z = np.polyfit(x, y, 1)\n",
    "        p = np.poly1d(z)\n",
    "        \n",
    "        # Add the line to the plot\n",
    "        ax.plot(x, p(x), \"r--\", label='Trendline')  # Add a label to the trendline\n",
    "        \n",
    "        # Increase the size of the axis ticks\n",
    "        plt.xticks(fontsize=14)  \n",
    "        plt.yticks(fontsize=14)  \n",
    "        \n",
    "        # Add titles & axis titles\n",
    "        plt.title('Scatter plot of test against predicted values of \\n compressive strength (CsMPa) ', fontsize=16) \n",
    "        plt.xlabel('Actual CsMPa', fontsize=14)\n",
    "        plt.ylabel('Predicted CsMPa', fontsize=14)\n",
    "\n",
    "        # Add legend\n",
    "        plt.legend(bbox_to_anchor=(0.95, 0.2))\n",
    "      \n",
    "        ax = plt.gca() # Set ax = the current axes\n",
    "        plt.minorticks_on()\n",
    "        ax.grid(False)\n",
    "        \n",
    "        # Adding a text box containing each error metric rounded to 4 decimal places\n",
    "        rmse = round(gVarErrorDf.loc[binary_int].iloc[2], 4)\n",
    "        r2 = round(gVarErrorDf.loc[binary_int].iloc[3], 4)\n",
    "        mae = round(gVarErrorDf.loc[binary_int].iloc[4], 4)\n",
    "        \n",
    "        text_box = f'RMSE = {rmse}\\nR^2 = {r2}\\nMAE = {mae}'\n",
    "        plt.text(0.05, 0.95, text_box, transform=ax.transAxes, verticalalignment='top', fontsize=12, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "        \n",
    "        textstr ='''\n",
    "This scatter plot shows the relationship between the actual and \n",
    "predicted values of Compressive Strength. A trendline is shown in red. \n",
    "\n",
    "Each point on the plot represents a data point. The position of\n",
    "a point on the x-axis represents its actual value, and the position on \n",
    "the y-axis represents its predicted value. Error metrics (4 dp.) are shown in a text box \n",
    "at the top left. \n",
    "\n",
    "Use the checkboxes to remove variables from the model, the plot will update and allow you\n",
    "to visualise the effect of removing combinations of variables on the error metrics.\n",
    "        '''\n",
    "        \n",
    "        # Add a text box containing the description of the plot\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(1.05, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    # Grid of checkboxes in a 4x2 grid\n",
    "    grid = [\n",
    "        [checkboxes[i] for i in range(j, j+4)]\n",
    "        for j in range(0, 8, 4)\n",
    "    ]\n",
    "\n",
    "    grid = [[checkboxes[i] for i in range(j, j + 4)] for j in range(0, 8, 4)]\n",
    "    ui = widgets.VBox([widgets.HBox(row) for row in grid])\n",
    "    out = widgets.interactive_output(update_lines, {checkbox.description: checkbox for checkbox in checkboxes})\n",
    "\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_effect_bar() -> None:\n",
    "    \"\"\"\n",
    "    This function generates an interactive bar chart, showing the effect of removing each variable on the performance metrics (RMSE, R^2, and MAE).\n",
    "    Data is taken from the global DataFrame `gVarErrorDf`, which is populated by the `var_performance` function.\n",
    "\n",
    "    A dropdown menu is used to select the performance metric to display. The bar chart shows the performance metric for each variable, as well as the baseline (i.e., when no variables are removed).\n",
    "\n",
    "    Parameters: None\n",
    "    Returns: None (displays a bar chart)\n",
    "    \"\"\"\n",
    "\n",
    "    # List of binary variable names\n",
    "    index = [bin(2**i) for i in range(8)] #index = ['0b1', '0b10'...\n",
    "    \n",
    "    # List of error types\n",
    "    error_types = ['RMSE', 'R^2', 'MAE']\n",
    "\n",
    "    def plot_bar_chart(error_type):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        global gVarNames\n",
    "\n",
    "        # Get the values for each variable from gVarErrorDf\n",
    "        values = []\n",
    "        for name in index:\n",
    "            row = gVarErrorDf.loc[gVarErrorDf['Drop index'] == name, error_type]\n",
    "            if len(row) > 0:\n",
    "                values.append(row.values[0])\n",
    "            else:\n",
    "                values.append(np.nan)  # replace None with np.nan\n",
    "        \n",
    "        #Plot the bar chart with variables on the x-axis and the error metric on the y-axis\n",
    "        plt.bar(gVarNames[0:8], values[::-1]) #Corrects order of names vs values, do to drop index being in reverse order\n",
    "        plt.title(f'Bar chart of {error_type} after removing variable')\n",
    "        plt.xlabel('Variable', fontsize=14, labelpad=10)\n",
    "        plt.ylabel(error_type, fontsize=14)\n",
    "\n",
    "        # Add a constant line for the baseline (no variables removed)\n",
    "        baseline = gVarErrorDf.loc[gVarErrorDf['Drop index'] == '0b0', error_type].values[0]\n",
    "        plt.axhline(y=baseline, color='r', linestyle='--')\n",
    "\n",
    "        plt.legend(['Baseline', 'Variable'])\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.grid(False)\n",
    "        ax = plt.gca()\n",
    "        ax.minorticks_on()\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "        #remove minor x axis ticks\n",
    "        ax.xaxis.set_minor_locator(plt.NullLocator())\n",
    "        \n",
    "        # Adding explanatory text\n",
    "        textstr = f\"\"\"\n",
    "This graph shows the effect of removing each variable on the {error_type}.\n",
    "The red dashed line represents the baseline (no variables removed).\n",
    "Use the dropdown menu to select the error type.\n",
    "Note: The higher the R^2, the better. The lower the RMSE and MAE, the better.\n",
    "\n",
    "As can be seen removing the variable AGE has the largest negative impact on the {error_type}.\n",
    "Thus AGE is the most important variable for making accurate predictions.\n",
    "        \"\"\"\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        plt.text(1.05, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "        plt.show()\n",
    "\n",
    "    # Create dropdown menu for error types\n",
    "    interact(plot_bar_chart, error_type=error_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(X_final: pd.DataFrame, grid: dict) -> None:\n",
    "    \"\"\"\n",
    "    Plots the learning curve for the random forest regressor model, before and after optimisation.\n",
    "\n",
    "    Parameters:\n",
    "    X_final (pd.DataFrame): The preprocessed training data after dropping variables that damage performance.\n",
    "    grid (dict): The best hyperparameter combination.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Create an unoptimised and optimised estimator\n",
    "    estimator = RandomForestRegressor()\n",
    "    optimised_estimator = RandomForestRegressor(**grid)\n",
    "\n",
    "    # Calculate the learning curve for both estimators\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, gX_train_preprocessed, gy_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    train_sizes_final, train_scores_final, test_scores_final = learning_curve(optimised_estimator, X_final, gy_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "    # Calculate the mean squared error for each training set size\n",
    "    train_scores_mean = -train_scores.mean(axis=1)\n",
    "    test_scores_mean = -test_scores.mean(axis=1)\n",
    "    train_scores_mean_final = -train_scores_final.mean(axis=1)\n",
    "    test_scores_mean_final = -test_scores_final.mean(axis=1)\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(train_sizes, train_scores_mean, label='Training error (Unoptimised)')\n",
    "    ax.plot(train_sizes, test_scores_mean, label='Validation error (Unoptimised)')\n",
    "    ax.plot(train_sizes_final, train_scores_mean_final, label='Training error (Optimised)')\n",
    "    ax.plot(train_sizes_final, test_scores_mean_final, label='Validation error (Optimised)')\n",
    "\n",
    "    # Set axis labels & title\n",
    "    ax.set_xlabel('Training set size')\n",
    "    ax.set_ylabel('Mean squared error')\n",
    "    ax.set_title('Learning Curve')\n",
    "\n",
    "    # Add minor ticks and gridlines\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.grid(False)\n",
    "    ax.minorticks_on()\n",
    "    ax.legend()\n",
    "\n",
    "    # Add text box with explanation\n",
    "    textstr = '''\n",
    "This graph shows the learning curve for both the optimised and unoptimised Random Forest Regressor model.\n",
    "The optimised model drops any variables found to have a negative impact on the performance of the model,\n",
    "with new hyperparameters found using a grid search. \n",
    "\n",
    "The optimised model has a lower training error and a lower validation error. \n",
    "This indicates that the optimised model is less overfitted and will improve with larger datasets.'''\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax.text(1.05, 0.95, textstr, transform=ax.transAxes, fontsize=14, verticalalignment='top', horizontalalignment='left', bbox=props)\n",
    "\n",
    "    # Adjust plot layout and display\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax.grid(False)\n",
    "    ax.minorticks_on()\n",
    "    ax.legend()\n",
    "    plt.subplots_adjust(right=0.85, bottom=0.15)  # Adjust the bottom parameter to move the plot up\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_histogram(y_pred_final: np.ndarray, bar_width: float) -> None:\n",
    "    \"\"\"\n",
    "    Plots an intoractive histogram of the errors in the final model's predictions.\n",
    "\n",
    "    Parameters: \n",
    "    y_pred_final (np.ndarray): The predicted values from the final model.\n",
    "    bar_width: The width of the bars in the histogram.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the errors\n",
    "    errors = gy_test - y_pred_final\n",
    "\n",
    "    # Fit a normal distribution to the data\n",
    "    mu, std = norm.fit(errors)\n",
    "\n",
    "    # Create a histogram\n",
    "    plt.hist(errors, bins=int((max(errors)-min(errors))/bar_width), density=True, label='Error')\n",
    "\n",
    "    # Plot the normal distribution curve fitted to the error data\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    plt.plot(x, p, 'k', linewidth=2, label='Normal Distribution')\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('Error (CsMPa)')\n",
    "    plt.ylabel('Frequency (Proability Density)')\n",
    "    plt.title('Histogram of Errors')\n",
    "\n",
    "    # Remove grid lines and add minor tick marks\n",
    "    plt.grid(False)\n",
    "    plt.minorticks_on()\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Add text box with explanation\n",
    "    textstr = '''\n",
    "This graph shows a histogram of the errors in the final model's predictions. \n",
    "The errors are calculated as the difference between the actual values (gy_test) and the predicted values (y_pred_final). \n",
    "The histogram shows the frequency of different error values.\n",
    "The black line shows a fitted normal distribution curve.'''\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    plt.text(1.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=14, verticalalignment='top', horizontalalignment='left', bbox=props)\n",
    "\n",
    "    # Show the histogram\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Function not in use \n",
    "# Used to visualise the random forest trees and as such non-esential\n",
    "# To use, the regressor must be returned from the random_forest_regression function\n",
    "\n",
    "def visualize_rf_trees(rf_model, num_trees=1):\n",
    "    \"\"\"\n",
    "    This function plots the first 'num_trees' trees in a Random Forest model.\n",
    "\n",
    "    Parameters:\n",
    "    rf_model (sklearn.ensemble.RandomForestRegressor): The trained Random Forest model.\n",
    "    num_trees (int): The number of trees to visualize. Default is 1.\n",
    "    \"\"\"\n",
    "    for i in range(num_trees):\n",
    "        plt.figure(figsize=(20,10))  # Set the figure size\n",
    "        plot_tree(rf_model.estimators_[i], filled=True)  # Plot the i-th decision tree\n",
    "        plt.show()\n",
    "\n",
    "#rf_model, y_pred = random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed)\n",
    "#visualize_rf_trees(rf_model, num_trees=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: main()\n",
    "    RUNTIME ~3:00 minutes\n",
    "\n",
    "The `main()` function is the entry point to the main program. It is responsible for coordinating the execution of other functions and controlling the flow of the program.\n",
    "- **NOTE: As the regressor model is trained more then once there are slight deviations between the error metrics displayed (eg. Optimisation & Dropping variables)**\n",
    "\n",
    "**This does not generate the final model or analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\" \n",
    "    This function calls all the other functions in the script and is used to facilitate the ML pipeline.\n",
    "\n",
    "    Parameters: None\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    X_dataset, y_dataset = csv_import() # Call csv_import function to import data\n",
    "    preprocessing(X_dataset, y_dataset) # Carrying out preprocessing, dropping duplicates and usng simple mean imputer and standard scaler\n",
    "    print(\"Preprocessing done: Standard Scaler, Simple (mean) Imputer\")\n",
    "\n",
    "    comp_graph() # Comparing the performance of the different models\n",
    "\n",
    "    best_model, df = regression_performance() # Identifying the best model, and printing the model name\n",
    "    print(f\"The best model is: {best_model} \\n\")\n",
    "\n",
    "    # Hyperparameter optimisation\n",
    "    print(\"Performing optimisation via randomised & grid search\")\n",
    "    best_grid_params = random_grid_search()\n",
    "    print(f\"Optimisation complete: Best parameters: {best_grid_params} \\n\\nError metrics are as follows:\")\n",
    "    best_grid_params = opt_unopt_random_forest(best_grid_params)\n",
    "\n",
    "    # Testing dropping variables from the model\n",
    "    var_performance(best_grid_params, best_model)\n",
    "    gVarErrorDf.to_excel('variable_drop_impact.xlsx') #export gVarErrorDf to excel\n",
    "    var_impact_df = var_impact_pct()\n",
    "\n",
    "    print(\"\\nSingle and double variable impact shown below - See Excel file (variable_drop_impact) for full results:\")\n",
    "    display(var_impact_df.head(5))\n",
    "    print(\n",
    "        \"\\nAge is the most important variable for making accurate predictions. \"\n",
    "        \"See Excel for more details.\\n\"\n",
    "        \"Use the interactive graphs below to see the effect of removing variables \"\n",
    "        \"on the error metrics.\\n\"\n",
    "    )\n",
    "    # Plotting interactive graphs\n",
    "    var_effect_line() \n",
    "    var_effect_bar()\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function: Final_run()\n",
    "    RUNTIME ~1:30 minutes\n",
    "\n",
    "This function is run to produce the final model, provide error analysis and plot the final learning curve compared to the unoptimised random forrest model.\n",
    "An interactive histogram of errors is also shown allowing the user to alter bar width (showing a normal distribution).\n",
    "\n",
    "**For the final run we decided that RMSE was the most critical metric, with large outliers potentially being catastrophic (bridge/building collapses etc.) so chose to drop variables to optimise RMSE above the other 2 metrics tested**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_run()-> None:\n",
    "    \"\"\"\n",
    "    This function runs the final model, dropping the variables that have a negative impact on the performance of the model (RMSE).\n",
    "    Learning curves are plotted for the final model, and a histogram of the errors is generated.\n",
    "    The mean error, standard deviation of errors and 95% confidence interval of errors are calculated.\n",
    "\n",
    "    Parameters: None\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    X_train_final, X_test_final = var_drop_final() # Variables are dropped based on RMSE ONLY\n",
    "    final_grid_params, y_pred_final = final_hyperparameters(X_train_final, X_test_final)\n",
    "    plot_learning_curve(X_train_final, final_grid_params)\n",
    "\n",
    "    # Create a slider for bar width\n",
    "    interact(error_histogram, y_pred_final=fixed(y_pred_final), bar_width=(0.1, 5.0, 0.1))\n",
    "    \n",
    "    print(\"Final model error analysis:\")\n",
    "    final_error_analysis(y_pred_final)\n",
    "final_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "[1] - S. Angioletti-Uberti, “Lecture Content,” can be found under https://bb.imperial.ac.uk/ultra/courses/_39564_1/cl/outline, 2024.\n",
    "\n",
    "[2] - “Correlation Matrix, Demystified. What is, how is it built and what is it… | by Giuseppe Mastrandrea | Towards Data Science,” can be found under https://towardsdatascience.com/correlation-matrix-demystified-3ae3405c86c1, n.d.\n",
    "\n",
    "[3] - “Regression Metrics for Machine Learning - MachineLearningMastery.com,” can be found under https://machinelearningmastery.com/regression-metrics-for-machine-learning/, n.d."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
