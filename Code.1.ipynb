{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year 2 Computational Challenge - Machine Learning\n",
    "    Douglas Penning, Roman Ogorodnov, Manish Saharan, Ryan Hutchings\n",
    "    \n",
    "Overview:\n",
    "\n",
    "We chose to break the programme into key sections, detailed by the headings shown throught:\n",
    "- Import & Preprocessing\n",
    "\n",
    "    - Data is imported from the .csv file, with the number of duplicate, and null entries counted. Removing the **23 duplicate entries** was found to improve the RMSE of the chosen model by **~0.5**. We chose to initially use **simple median imputer** and **standard scaler** to ensure a consistent and computationally simple approach allowing each model to be tested using the same datasets.\n",
    "\n",
    "\n",
    "    - We chose to use an 80-20 test-train split and created the global variables **gX_train_preprocessed, gy_test etc** which are used throughout\n",
    "\n",
    "    - **Note:** While as a group we did discuss the posability of removing variables initially, while we accepted there was likley a correlation between many of them eg course & fine aggregate, we felt that it would be a more rigourous approach to iteratively test each dropped combination and then retrain a final model using the new dataset if required\n",
    "\n",
    "- Model selection\n",
    "\n",
    "    - We begin by defining each regression function (using a function for each allowed us to quickly individually test & efficiently pass through hyperparameters etc), using the standard parameters in scikit-learn and thus having the default best_params = {}. After each regressor is created the global model name dictionary is defined\n",
    "\n",
    "    - Models were tested unoptimised to reduce computational expense, we chose to use 3 error types:\n",
    "        * **Root Mean Square Error** - Giving differences between actual and predicted values and actual, with the ^2 giving greater weighting to larger errors.\n",
    "        * **R^2** - Measuring correlation between datapoints, used as it is easily visually observed in the plots \n",
    "        * **Mean Absolute Average Error** - While similar to RMSE, MAE is less sensitive to larger outliers*\n",
    "    - Models were then ranked on each error type (accounting for the possability for models performing better/worse in specific error types) with **Random Forrest** achieving the best score. **Note:** We have tried to ensure this code is resilient, and as such have used best_model and the model dictionary where possible in case a different dataset is better suited to a different model (though this was not possible in the optimisation section).\n",
    "\n",
    "- Optimisation\n",
    "    - We employed randomised serch initially though found that results were highly variable so chose GridSearch. Initial runs were carried out on larger prameter grids, taking many hours to run. The grid used in this code covers the best parameters found.\n",
    "        \n",
    "- Dropping variables\n",
    "    - We chose to drop all possible combinations of variables using an 8 bit binary number as an index, allowing us to analyse any correlation and it's effect on model performance. Depending on model optimisation it was found that removing course & fine aggregates produced improved results.\n",
    "\n",
    "    - 2 datafiles are produced, showing the binary drop_index and errors, and a second *.xlsx* using the variable names (for single and pairs) allowing easier analysis.\n",
    "\n",
    "- Graphing & Final data presentation\n",
    "    - Comparitive graphs are shown for each regression model, allowing quick visual comparison \n",
    "    - An interactive line graph, for the chosen model, allows the user to deselect  variables to see the effect on correlation and errors quickly. This uses the gVarErrorDF meaning no extra computation is required\n",
    "    - An interactive bar chart shows the effect removing individual errors has on each error metric, with a baseline shown in red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries\n",
    "This section imports all libraries utilised within the code:\n",
    " - Misc including pandas & numpy etc.\n",
    " - scikit-learn modules\n",
    " - Graphing libraries including matplotlib, as well as ipywidgets and seaborn for interactivity/formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# SKLearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "# Graphing\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from ipywidgets import interact, widgets, interactive\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "\n",
    "\n",
    "# Global Variables\n",
    "global gMethodDictionary\n",
    "global gVarErrorDf\n",
    "gVarErrorDf = pd.DataFrame(columns=[ 'Drop index','Predicted y', 'RMSE', 'R^2', 'MAE', 'RMSE %', 'R^2 %', 'MAE %'])\n",
    "gVarNames = ['Cement','Blast Furnace Slag','Fly Ash','Water','Superplasticizer','Coarse Aggregate','Fine Aggregate','Age','Concrete compressive strength',' ']\n",
    "gX_train_preprocessed = pd.DataFrame()\n",
    "gX_test_preprocessed = pd.DataFrame()\n",
    "gy_train = []\n",
    "gy_test = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import, handling and preprocessing\n",
    " - Imports, drops duplicated rows, seperates training/target columns, completes basic preprocessing (see overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_import():\n",
    "    #import data from the files\n",
    "    dataset = pd.read_csv('Concrete_Data_Yeh_final.csv')\n",
    "\n",
    "    #Data Preprocessing\n",
    "    #format as a dataframe\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    \n",
    "    print(f'Null values: \\n', dataset.isnull().sum()) #check for null values\n",
    "    print(f'Duplicated values:',dataset.duplicated().sum()) #check for duplicates\n",
    "    dataset = dataset.drop_duplicates() #drop duplicates\n",
    "    dataset.dtypes #check for data types\n",
    "  \n",
    "    y = dataset[\"csMPa\"]\n",
    "    X = dataset.drop(\"csMPa\", axis=1)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X,y):\n",
    "    # Splitting the data into training and test sets\n",
    "    global gX_train_preprocessed, gX_test_preprocessed\n",
    "    global gy_train, gy_test    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Creating a preprocessing pipeline that imputes missing values with the mean \n",
    "    # and scales features to have zero mean and unit variance.\n",
    "    preprocessing_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    gX_train_preprocessed = pd.DataFrame(preprocessing_pipeline.fit_transform(X_train))\n",
    "    gX_test_preprocessed = pd.DataFrame(preprocessing_pipeline.transform(X_test))\n",
    "    gy_train = y_train\n",
    "    gy_test = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression functions\n",
    "\n",
    "This section defines functions for various regression models which are later compared to choose the most effective model.\n",
    "\n",
    "The regression models included are:\n",
    "\n",
    "1. **Linear Regression (`linear_regression`)**: While it is stated that data is nonlinear we used it as a baseline.\n",
    "\n",
    "2. **Decision Tree Regression (`decision_tree_regression`)**\n",
    "\n",
    "3. **Random Forest Regression (`random_forest_regression`)**: **Chosen model**\n",
    "\n",
    "4. **Lasso Regression (`lasso_regression`)**\n",
    "\n",
    "5. **Elastic Net Regression (`elastic_net_regression`)**\n",
    "\n",
    "6. **Ridge Regression (`ridge_regression`)**\n",
    "\n",
    "7. **Support Vector Regression (`svr_regression`)**\n",
    "\n",
    "8. **K-Nearest Neighbors Regression (`knn_regression`)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(xTrainData: pd.DataFrame, yTrainData: pd.DataFrame, yTestData: pd.Series, bestFit = {}):\n",
    "    \"\"\"\n",
    "    This function creates and fits a linear regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (numpy.ndarray or pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (numpy.ndarray or pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (numpy.ndarray or pandas.DataFrame): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The predicted values of the dependent variable for the test set.\n",
    "    \"\"\" \n",
    "   \n",
    "    regressor = LinearRegression(**bestFit) # Creating the Linear Regression model\n",
    "    regressor.fit(xTrainData, yTrainData) # Fitting the data\n",
    "    y_pred = regressor.predict(yTestData) # Making predictions\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_regression(xTrainData: pd.DataFrame, yTrainData: pd.DataFrame, yTestData: pd.Series, bestFit = {}):\n",
    "    \"\"\"\n",
    "    This function creates and fits a decision tree regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (numpy.ndarray or pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (numpy.ndarray or pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (numpy.ndarray or pandas.DataFrame): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The predicted values of the dependent variable for the test set.\n",
    "    \"\"\" \n",
    "\n",
    "    regressor = DecisionTreeRegressor(**bestFit) # Creating the Decision Tree regressor\n",
    "    regressor.fit(xTrainData, yTrainData)\n",
    "    y_pred = regressor.predict(yTestData)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_regression(xTrainData: pd.DataFrame, yTrainData: pd.DataFrame, yTestData: pd.Series, bestFit = {}):\n",
    "    \"\"\"\n",
    "    This function creates and fits a random forest regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (numpy.ndarray or pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (numpy.ndarray or pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (numpy.ndarray or pandas.DataFrame): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The predicted values of the dependent variable for the test set.\n",
    "    \"\"\" \n",
    "   \n",
    "    regressor = RandomForestRegressor(**bestFit) # Creating the Random Forest Regressor\n",
    "    regressor.fit(xTrainData, yTrainData) \n",
    "    y_pred = regressor.predict(yTestData)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lasso(xTrainData: pd.DataFrame, yTrainData: pd.DataFrame, yTestData: pd.Series, bestFit = {}):\n",
    "    \"\"\"\n",
    "    This function creates and fits a Lasso regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (numpy.ndarray or pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (numpy.ndarray or pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (numpy.ndarray or pandas.DataFrame): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The predicted values of the dependent variable for the test set.\n",
    "    \"\"\" \n",
    "    \n",
    "    model = Lasso(**bestFit) # Create and train the model\n",
    "    model.fit(xTrainData, yTrainData)\n",
    "    y_pred = model.predict(yTestData)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_net_regression(xTrainData: pd.DataFrame, yTrainData: pd.DataFrame, yTestData: pd.Series, bestFit = {}):\n",
    "    \"\"\"\n",
    "    This function creates and fits a elastic net regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (numpy.ndarray or pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (numpy.ndarray or pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (numpy.ndarray or pandas.DataFrame): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The predicted values of the dependent variable for the test set.\n",
    "    \"\"\" \n",
    "\n",
    "    regressor = ElasticNet(**bestFit)    # Creating the Elastic Net Regressor\n",
    "    regressor.fit(xTrainData, yTrainData)\n",
    "    y_pred = regressor.predict(yTestData)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(xTrainData: pd.DataFrame, yTrainData: pd.DataFrame, yTestData: pd.Series, bestFit = {}):\n",
    "    \"\"\"\n",
    "    This function creates and fits a ridge regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (numpy.ndarray or pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (numpy.ndarray or pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (numpy.ndarray or pandas.DataFrame): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The predicted values of the dependent variable for the test set.\n",
    "    \"\"\" \n",
    "\n",
    "    regressor = Ridge(**bestFit) # Creating the Ridge Regressor\n",
    "    regressor.fit(xTrainData, yTrainData)\n",
    "    y_pred = regressor.predict(yTestData)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_svr(xTrainData: pd.DataFrame, yTrainData: pd.DataFrame, yTestData: pd.Series, bestFit = {}):\n",
    "    \"\"\"\n",
    "    This function creates and fits a support vector regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (numpy.ndarray or pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (numpy.ndarray or pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (numpy.ndarray or pandas.DataFrame): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The predicted values of the dependent variable for the test set.\n",
    "    \"\"\" \n",
    "   \n",
    "    model = SVR(**bestFit)  # Create and train the model\n",
    "    model.fit(xTrainData, yTrainData)\n",
    "    y_pred = model.predict(yTestData)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_knn(xTrainData: pd.DataFrame, yTrainData: pd.DataFrame, yTestData: pd.Series, bestFit = {}):\n",
    "    \"\"\"\n",
    "    This function creates and fits a KNN regression model\n",
    "\n",
    "    Parameters:\n",
    "    xTrainData (numpy.ndarray or pandas.DataFrame): The independent variables for training\n",
    "    yTrainData (numpy.ndarray or pandas.Series): Target training values (Compressive Strength)\n",
    "    yTestData (numpy.ndarray or pandas.DataFrame): The independent variables for the test set, used to predict y_pred\n",
    "    bestFit (dict, optional): The parameters for the Linear Regression model. Defaults to an empty dictionary.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The predicted values of the dependent variable for the test set.\n",
    "    \"\"\" \n",
    "\n",
    "    model = KNeighborsRegressor(**bestFit)    # Create and train the model\n",
    "    model.fit(xTrainData, yTrainData)\n",
    "    y_pred = model.predict(yTestData)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a dictionary containing Name:Function pairs for each regression method\n",
    "gMethodDictionary = [('Linear Regression', linear_regression),\n",
    "                    ('Decision Tree Regression', decision_tree_regression),\n",
    "                    ('Random Forest Regression', random_forest_regression),\n",
    "                    ('Ridge', ridge_regression), \n",
    "                    ('Lasso', perform_lasso),\n",
    "                    ('Elastic Net', elastic_net_regression),\n",
    "                    ('Support Vector Regression', perform_svr),\n",
    "                    ('K-Nearest Neighbors Regression', perform_knn)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Analysing the performance of each regression model to determine which technique most accuratley predicts compressive strength\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_performance():\n",
    "    \"\"\"\n",
    "    This function evaluates the performance of all regression models defined, iterating through the global `gMethodDictionary`.\n",
    "    Predicted values are used to calculate the root mean squared error (RMSE), R^2 score, and mean absolute error (MAE).\n",
    "    The results are stored in a DataFrame.\n",
    "\n",
    "    Models are ranked based on each performance metric, with the highest R^2 score and the lowest \n",
    "    RMSE and MAE being the best. It calculates a rank sum for each model and identifies the model with the lowest \n",
    "    rank sum as the best model.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    best_model (str): The name of the best model.\n",
    "    df (pd.DataFrame): A DataFrame containing the performance metrics and ranks for each model.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    for model_name, model in gMethodDictionary: # Loop over each model\n",
    "        # Calling each regression model, passing in the training and test data (Note: models are unoptimised so bestFit assumes default parameters))      \n",
    "        y_pred = model(gX_train_preprocessed, gy_train, gX_test_preprocessed)\n",
    "\n",
    "        # Calculate the performance metrics\n",
    "        rmse = sqrt(mean_squared_error(gy_test, y_pred))\n",
    "        r2 = r2_score(gy_test, y_pred)\n",
    "        mae = mean_absolute_error(gy_test, y_pred)\n",
    "    \n",
    "        # Append the data to the list\n",
    "        data.append([model_name, y_pred, rmse, r2, mae])\n",
    "\n",
    "    # Create a DataFrame from the data\n",
    "    df = pd.DataFrame(data, columns=['Model', 'y_pred', 'RMSE', 'R^2','MAE'])\n",
    "\n",
    "    # Set the model names as the index\n",
    "    df.set_index('Model', inplace=True)\n",
    "\n",
    "    # Rank each metric, with the highest being the best for R^2 and the lowest being the best for RMSE and MAE\n",
    "    df['R^2_rank'] = df['R^2'].rank(ascending=False)\n",
    "    df['RMSE_rank'] = df['RMSE'].rank()\n",
    "    df['MAE_rank'] = df['MAE'].rank()\n",
    "\n",
    "    # Calculate the sum of the ranks\n",
    "    df['rank_sum'] = df['R^2_rank'] + df['RMSE_rank'] + df['MAE_rank'] \n",
    "\n",
    "    # Find the model with the lowest rank sum\n",
    "    best_model = df['rank_sum'].idxmin()\n",
    "\n",
    "    return best_model, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperperameters\n",
    "\n",
    "As shown Random Forrest Regression has the best untuned performance on our dataset and as such we are only analysing hyperperameters for this model:\n",
    "The most important hyperparameters to tune in a Random Forest model to improve its performance are:\n",
    "\n",
    "1. `n_estimators`: This is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees give you better performance but makes your code slower.\n",
    "\n",
    "2. `max_depth`: The maximum depth of the tree. This parameter can help to prevent overfitting. If the max depth is too high, the model may learn too much from the training data and perform poorly on unseen data.\n",
    "\n",
    "3. `min_samples_split`: The minimum number of samples required to split an internal node. If you increase this parameter, each tree in the forest becomes more constrained as it has to consider more samples at each node.\n",
    "\n",
    "4. `min_samples_leaf`: The minimum number of samples required to be at a leaf node. This parameter is similar to min_samples_splits, however, this describe the minimum number of samples of samples at the leafs, the base of the tree.\n",
    "\n",
    "5. `max_features`: The number of features to consider when looking for the best split. If set to \"auto\", then `max_features=sqrt(n_features)`.\n",
    "\n",
    "6. `bootstrap`: Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n",
    "\n",
    "Both Random Search and Grid Search are hyperparameter tuning techniques, and each has its own advantages and disadvantages.\n",
    "\n",
    "**Grid Search** systematically works through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. The benefit is that it's guaranteed to find the best combination of parameters supplied. However, it can be computationally expensive, especially if the number of parameters or their possible values are large.\n",
    "\n",
    "**Random Search** sets up a grid of hyperparameter values and selects random combinations to train the model and score. The benefit is that it's not as computationally expensive as Grid Search, and you have more control over how long you want it to run for, as you can set the number of iterations. However, it's not guaranteed to find the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search():\n",
    "    \"\"\"\n",
    "    This function performs a grid search to identify the best hyperparameter combination for the Random Forest Regressor model.\n",
    "    The function first defines a hyperparameter grid, then performs a search to identify the best hyperparameter combination. \n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    best_grid_params (dict): The best hyperparameter combination.\n",
    "    \"\"\"\n",
    "\n",
    "    #Parameters used for initial long run shown below, initial run took over 12 hours to complete so parameters were narrowed to a range covering the \n",
    "    #best parameters and shown to demonstrate the process. This was tested using randomised search as the full grid search would compute 1,135,104 combinations taking over\n",
    "    #15 days to complete.\n",
    "    \"\"\" \n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 125, 135, 135, 155, 165, 200, 300],\n",
    "        'max_depth': [None,5, 20, 30, 40, 50],\n",
    "        'min_samples_split': [ 1, 2, 3, 4, 5, 10],\n",
    "        'min_samples_leaf': [ 1, 2, 5, 10],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'bootstrap': [True, False],\n",
    "        'max_leaf_nodes': [None, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90],\n",
    "        'min_impurity_decrease': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    }\"\"\"\n",
    "    # With this parameter grid, optimised values have been found to be:\n",
    "    # Optimized RMSE: 4.081712201661637\n",
    "    # Optimized R^2: 0.9273305050035258\n",
    "    # Optimized MAE: 2.9955957095709524\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [150, 155, 160],\n",
    "        'max_depth': [25, 30, 35],\n",
    "        'min_samples_split': [2, 3, 4],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'bootstrap': [False]}\n",
    "\n",
    "    # Step 5: Perform grid search\n",
    "    rf_grid = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid, cv=5, verbose = 1)\n",
    "    rf_grid.fit(gX_train_preprocessed, gy_train)\n",
    "\n",
    "    # Step 6: Identify the best hyperparameter combination from grid search\n",
    "    best_grid_params = rf_grid.best_params_\n",
    "\n",
    "    return best_grid_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_unopt_random_forest(best_grid_params: dict):\n",
    "    \"\"\"\n",
    "    This function compares the performance of the original unoptimised Random Forest Regressor model with the optimised Random Forest Regressor model.\n",
    "    The model trained, using the hyperparameter combination identified by the grid search, performance metrics are compared to the original model. If the\n",
    "    optimised model performs worse than the original model, the grid search is deemed unsuccessful and an empty dictionary is returned.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    best_grid_params (dict): The best hyperparameter combination.\n",
    "    \"\"\" \n",
    "    # Original unoptimized Random Forest Regression\n",
    "    y_pred_unoptimized = random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed)\n",
    "\n",
    "    # Optimized Random Forest Regression  \n",
    "    y_pred_optimized = random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed,best_grid_params)\n",
    "\n",
    "    # Calculate errors\n",
    "    rmse_unoptimised = sqrt(mean_squared_error(gy_test, y_pred_unoptimized))\n",
    "    rmse_optimised = sqrt(mean_squared_error(gy_test, y_pred_optimized))\n",
    "    r2_unoptimised = r2_score(gy_test, y_pred_unoptimized)\n",
    "    r2_optimised = r2_score(gy_test, y_pred_optimized)\n",
    "    mae_unoptimised = mean_absolute_error(gy_test, y_pred_unoptimized)\n",
    "    mae_optimised = mean_absolute_error(gy_test, y_pred_optimized)\n",
    "\n",
    "    # Print errors\n",
    "    print(\"Unoptimized RMSE:\", rmse_unoptimised)\n",
    "    print(\"Optimized RMSE:\", rmse_optimised)\n",
    "    print(\"Unoptimized R^2:\", r2_unoptimised)\n",
    "    print(\"Optimized R^2:\", r2_optimised)\n",
    "    print(\"Unoptimized MAE:\", mae_unoptimised)\n",
    "    print(\"Optimized MAE:\", mae_optimised)\n",
    "\n",
    "    if rmse_optimised > rmse_unoptimised:\n",
    "        print(\"Optimisation unsuccessful\")\n",
    "        best_grid_params = {}\n",
    "    return best_grid_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping variables\n",
    "\n",
    "    var_drop():\n",
    "Used to create a new data set, dropping all combinations of variables using a binary 'drop index'. This allows us to determine the effect on regression performance when dropping single, pairs, triplets or any other combination of variables. This was chosen to allow us to clearly see and account for variable correlation, and efficiently plot the interactive graphs as shown.\n",
    "\n",
    "    var_performance():\n",
    "Taking the dataset from var_drop(): to measure the performance effect of dropping variables\n",
    "\n",
    "    var_impact_pct():\n",
    "Calculating the difference in original vs retrained performance for various combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_drop(dropVar: int):\n",
    "    \"\"\"\n",
    "    Drops columns from the training and testing datasets based on a binary number.\n",
    "\n",
    "    Each digit in the binary number corresponds to a column in the datasets. If the digit is 1, the corresponding column is dropped. If the digit is 0, the corresponding column is kept.\n",
    "\n",
    "    Parameters:\n",
    "    dropVar (int): An intiger, converted to binary with each digit representing a column in the datasets.\n",
    "\n",
    "    Returns:\n",
    "    datasetTrain_drop (pd.DataFrame): The training dataset with the columns dropped.\n",
    "    datasetTest_drop (pd.DataFrame): The testing dataset with the columns dropped.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the binary number to a list of booleans\n",
    "    bool_list = [bool(int(x)) for x in format(dropVar, '0{}b'.format(len(gX_train_preprocessed.columns)))]\n",
    "\n",
    "    # Create a list of column names to drop based on the boolean list\n",
    "    columns_to_drop_train = gX_train_preprocessed.columns[bool_list]\n",
    "    columns_to_drop_test = gX_test_preprocessed.columns[bool_list]\n",
    "\n",
    "    # Drop the columns\n",
    "    datasetTrain_drop = gX_train_preprocessed.drop(columns_to_drop_train, axis=1)\n",
    "    datasetTest_drop = gX_test_preprocessed.drop(columns_to_drop_test, axis=1)\n",
    "\n",
    "    return datasetTrain_drop, datasetTest_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_performance(best_grid_params: dict):\n",
    "    \"\"\"\n",
    "    This function evaluates the performance of the best regression model for every combination of dropped variables.\n",
    "\n",
    "    The function first identifies the best regression model by calling the `regression_performance` function. \n",
    "    It then loops over all variables in the preprocessed training and testing data, dropping one variable at a time \n",
    "    and evaluating the performance of the model without that variable. \n",
    "\n",
    "    The function also calculates the variation of each performance metric compared to the base case (i.e., when no \n",
    "    variables are dropped). The results are stored in the global DataFrame `gVarErrorDf`.\n",
    "\n",
    "    Parameters:    None\n",
    "    Returns:    None\n",
    "    \"\"\"\n",
    "    x_train_drop = gX_train_preprocessed\n",
    "    x_test_drop = gX_test_preprocessed\n",
    "    best_model, df = regression_performance() # Replace with a pass in parameter\n",
    "\n",
    "    for key, model_function in gMethodDictionary:\n",
    "        if key == best_model:\n",
    "            break\n",
    "\n",
    "    for i in range(255):\n",
    "        x_train_drop, x_test_drop = var_drop(i)\n",
    "       \n",
    "        y_pred = model_function(x_train_drop, gy_train, x_test_drop, best_grid_params)\n",
    "        rms_error = sqrt(mean_squared_error(gy_test, y_pred))\n",
    "        r2_error = r2_score(gy_test, y_pred)\n",
    "        ma_error = mean_absolute_error(gy_test, y_pred)\n",
    "        if i == 0:\n",
    "            rms_var = 1\n",
    "            r2_var = 1\n",
    "            ma_var = 1\n",
    "            rms_base = rms_error\n",
    "            r2_base = r2_error\n",
    "            ma_base = ma_error\n",
    "\n",
    "        else:\n",
    "            rms_var = rms_error / rms_base\n",
    "            r2_var = r2_error / r2_base\n",
    "            ma_var = ma_error / ma_base\n",
    "\n",
    "\n",
    "        gVarErrorDf.loc[i] = [bin(i), y_pred, rms_error, r2_error, ma_error, rms_var, r2_var, ma_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_impact_pct():\n",
    "    \"\"\"\n",
    "    This function calculates the impact of each variable on the model's performance metrics (RMSE, R^2, and MAE) \n",
    "    when the variable is dropped. It creates a DataFrame that contains the performance metrics for each variable \n",
    "    when it's dropped, as well as when no variables are dropped. It also calculates the impact of dropping two \n",
    "    variables at a time. \n",
    "\n",
    "    The function then ranks each model (i.e., each set of dropped variables) based on the performance metrics, \n",
    "    calculates a rank sum for each model, and sorts the DataFrame by the rank sum. The model with the lowest rank \n",
    "    sum is considered the best model.\n",
    "\n",
    "    The DataFrame is printed to the console and also exported to an Excel file named 'impact_df.xlsx'.\n",
    "\n",
    "    Parameters:    None\n",
    "\n",
    "    Returns:    None\n",
    "    \"\"\"\n",
    "\n",
    "    impact_df = pd.DataFrame(columns=['Dropped variable: 1','Dropped variable : 2', 'RMSE', 'R^2', 'MAE','RMSE %', 'R^2 %', 'MAE %', ])\n",
    "\n",
    "    #Single variable impact\n",
    "    for i in range(8):\n",
    "        index = 2**i\n",
    "        impact_df.loc[i] = [gVarNames[(7-i)],gVarNames[9], gVarErrorDf.loc[index]['RMSE'], gVarErrorDf.loc[index]['R^2'], gVarErrorDf.loc[index]['MAE'], gVarErrorDf.loc[index]['RMSE %'], gVarErrorDf.loc[index]['R^2 %'], gVarErrorDf.loc[index]['MAE %']]\n",
    "    #Data frame containing all single variable impacts\n",
    "    \n",
    "    impact_df.loc[8] = [\"None dropped\",'', gVarErrorDf.loc[0]['RMSE'], gVarErrorDf.loc[0]['R^2'], gVarErrorDf.loc[0]['MAE'], gVarErrorDf.loc[0]['RMSE %'], gVarErrorDf.loc[0]['R^2 %'], gVarErrorDf.loc[0]['MAE %']] \n",
    "    \n",
    "    #double variable impact\n",
    "    l = 9\n",
    "    for j in range(7):\n",
    "        #loop from j+1 to 8\n",
    "        for k in range(j+1,8):\n",
    "            if j != k:\n",
    "                index = 2**j + 2**k #7- index to move through gVarNames correctly\n",
    "                impact_df.loc[l] = [gVarNames[(7-j)], gVarNames[(7-k)], gVarErrorDf.loc[index]['RMSE'], gVarErrorDf.loc[index]['R^2'], gVarErrorDf.loc[index]['MAE'], gVarErrorDf.loc[index]['RMSE %'], gVarErrorDf.loc[index]['R^2 %'], gVarErrorDf.loc[index]['MAE %']]\n",
    "                l += 1\n",
    "    #Data frame containing all double variable impacts'''\n",
    "    \n",
    "    # Rank each metric, with the highest being the best for R^2 and the lowest being the best for RMSE and MAE\n",
    "    impact_df['R^2_rank'] = impact_df['R^2'].rank(ascending=False)\n",
    "    impact_df['RMSE_rank'] = impact_df['RMSE'].rank()\n",
    "    impact_df['MAE_rank'] = impact_df['MAE'].rank()\n",
    "\n",
    "    # Calculate the sum of the ranks\n",
    "    impact_df['rank_sum'] = impact_df['R^2_rank'] + impact_df['RMSE_rank'] + impact_df['MAE_rank']\n",
    "\n",
    "    # Find the model with the lowest rank sum\n",
    "    best_model = impact_df['rank_sum'].idxmin()\n",
    "    \n",
    "    impact_df.sort_values(by=['rank_sum'], inplace=True) # Sort the data frame based on the rank sum\n",
    "    impact_df.to_excel('impact_df.xlsx')#export to excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Predictions\n",
    "    \n",
    "Using all information gathered to create a retrained model (Found to be Random Forrest Regression) removing variables found to harm performance. Hyperparameters are re-optimised for this new data set. Results are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping variables found previously to have a negative impact on the model performance\n",
    "def var_drop_final():\n",
    "    \"\"\"\n",
    "    Drops columns from the original dataset based on a binary number.\n",
    "\n",
    "    Each digit in the binary number corresponds to a column in the dataset. If the digit is 1, the corresponding column is dropped. If the digit is 0, the corresponding column is kept.\n",
    "\n",
    "    Parameters:\n",
    "    dropVar (int): A binary number where each digit represents a column in the dataset.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The modified dataset with specified columns dropped. \n",
    "    \"\"\"\n",
    "    #sort gVarError by RMSE\n",
    "    varError_sort = gVarErrorDf.sort_values(by=['RMSE'], ascending = True,  inplace=False)\n",
    "    drop_string = varError_sort['Drop index'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the scaling and impuation of the data using the specific dropped variables\n",
    "def scale_impute_final():\n",
    "    #test each combination of imputer & scaler to find the best combination\n",
    "    imputer = {\n",
    "        \"simple_imputer\": SimpleImputer(strategy='mean'),\n",
    "        'simple_imputer_median': SimpleImputer(strategy='median'),\n",
    "        'simple_imputer_most_frequent': SimpleImputer(strategy='most_frequent'),\n",
    "        'simple_imputer_constant': SimpleImputer(strategy='constant', fill_value=0),\n",
    "        'iterative': IterativeImputer(max_iter=10, random_state=0),\n",
    "        'mice': IterativeImputer(max_iter=10, random_state=0, estimator=BayesianRidge())\n",
    "        }\n",
    "\n",
    "    scaler = {\n",
    "        'standard_scaler': StandardScaler(),\n",
    "        'min_max_scaler': MinMaxScaler(),\n",
    "        'max_abs_scaler': MaxAbsScaler(),\n",
    "        'robust_scaler': RobustScaler(),\n",
    "        'quantile_transformer_normal': QuantileTransformer(n_quantiles=700, output_distribution='normal'),\n",
    "        'quantile_transformer_uniform': QuantileTransformer(n_quantiles=700, output_distribution='uniform'),\n",
    "        'power_transformer': PowerTransformer(),\n",
    "        'normalizer': Normalizer(),\n",
    "        'binarizer': Binarizer()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def final_plot_predictions(final_model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Plots y_pred vs y_test using the final model and displays a histogram of errors in y_pred.\n",
    "\n",
    "    Parameters:\n",
    "    final_model: The final trained model.\n",
    "    X_test: The preprocessed testing dataset.\n",
    "    y_test: The target variable for the testing dataset.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot y_pred vs y_test using the final model\n",
    "    y_pred_final = final_model.predict(X_test)\n",
    "\n",
    "    plt.scatter(y_test, y_pred_final)\n",
    "    plt.xlabel('y_test')\n",
    "    plt.ylabel('y_pred')\n",
    "    plt.title('y_pred vs y_test using the final model')\n",
    "    plt.show()\n",
    "\n",
    "    errors = y_test - y_pred_final\n",
    "\n",
    "    plt.hist(errors, bins=10)\n",
    "    plt.xlabel('Errors')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Errors in y_pred')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, X, y):\n",
    "    \"\"\"\n",
    "    Plots the learning curve for an estimator.\n",
    "\n",
    "    Parameters:\n",
    "    estimator: The estimator model.\n",
    "    X: The preprocessed training dataset.\n",
    "    y: The target variable for the training dataset.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "    train_scores_mean = -train_scores.mean(axis=1)\n",
    "    test_scores_mean = -test_scores.mean(axis=1)\n",
    "\n",
    "    plt.plot(train_sizes, train_scores_mean, label='Training error')\n",
    "    plt.plot(train_sizes, test_scores_mean, label='Validation error')\n",
    "    plt.xlabel('Training set size')\n",
    "    plt.ylabel('Mean squared error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalModel_histogram():\n",
    "    \"\"\"\n",
    "    Plots a histogram of the errors in the final model's predictions.\n",
    "\n",
    "    Parameters:\n",
    "    None\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Calculate the errors\n",
    "    errors = gy_test - (random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed))\n",
    "\n",
    "    # Fit a normal distribution to the data\n",
    "    mu, std = norm.fit(errors)\n",
    "\n",
    "    # Create a histogram\n",
    "    plt.hist(errors, bins=10, density=True)\n",
    "\n",
    "    # Plot the PDF\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    plt.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Errors')\n",
    "\n",
    "    #remove grid lines and add minor tick marks\n",
    "    plt.grid(False)\n",
    "    plt.minorticks_on()\n",
    "\n",
    "    # Show the histogram\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Final error analysis\n",
    "\n",
    "def final_error_analysis():\n",
    "    # calculates uncertainties in the final model's predictions\n",
    "    errors = gy_test - (random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed))\n",
    "    print(\"Mean error:\", np.mean(errors))\n",
    "    print(\"Standard deviation of errors:\", np.std(errors))\n",
    "    print(\"95% confidence interval of errors:\", np.percentile(errors, [2.5, 97.5]))\n",
    "\n",
    "    # calculates the percentage of predictions that are within 10% of the actual values\n",
    "    errors = abs(errors)\n",
    "    print(\"Percentage of predictions that are within 10% of the actual values:\", sum(errors < 0.1 * gy_test) / len(errors))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Functions\n",
    "\n",
    "1. `comp_graph()` - *Static*\n",
    "\n",
    "    This function creates subplots allowing visual comparison between all regression functions.\n",
    "\n",
    "2. `var_effect_line()` - *Interactive*\n",
    "\n",
    "    This function is used to visualise the effect of a variable on the outcome. The user can choose any combination of variables to drop by deselecting a checkbox, the graph shows predicted vs actual values of Compressive strength, and the error metrics are shown rounded to 4 decimal places.\n",
    "\n",
    "3. `var_effect_bar()` - *Interactive*\n",
    "\n",
    "    This function is used to create a bar chart showing the effect on the various errors, of removing individual variables from the dataset. Users can choose the error type from the drop down menu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_graph():\n",
    "    \"\"\"\n",
    "    This function generates a comparative graph of actual vs predicted values for different regression models.\n",
    "\n",
    "    The function calls the `regression_performance` function to get the best model and a DataFrame containing \n",
    "    the predictions of each model. It then creates a scatter plot for each model, comparing the actual and \n",
    "    predicted values. The scatter plots are arranged in a grid, with one subplot for each model with a y=x line added.\n",
    "\n",
    "    The function doesn't return anything. The graph is displayed using `plt.show()`.\n",
    "\n",
    "    Note: This function relies on the global variable `gy_test` for the actual values. Make sure this variable \n",
    "    is defined and properly initialized before calling this function.\n",
    "    \"\"\"\n",
    "    # Number of models taking the data from the data frame in the regression performance function\n",
    "    best_model, df = regression_performance()\n",
    "    n_models = len(df)\n",
    "\n",
    "    # Create a figure and axes with a subplot for each model\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(20, 15))\n",
    "\n",
    "    # Flatten the axes array\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # Loop over each model\n",
    "    for i, (model_name, row) in enumerate(df.iterrows()):         # Plot y_test vs y_pred\n",
    "        axs[i].scatter(gy_test, row['y_pred'], s=10)\n",
    "        axs[i].plot([gy_test.min(), gy_test.max()], [gy_test.min(), gy_test.max()], 'k--', lw=2)\n",
    "        axs[i].set_xlabel('Actual CsMPa', fontsize=12)\n",
    "        axs[i].set_ylabel('Predicted CsMPa', fontsize=12)\n",
    "        axs[i].set_title(f'{model_name}')\n",
    "        axs[i].xaxis.set_minor_locator(AutoMinorLocator())\n",
    "        axs[i].yaxis.set_minor_locator(AutoMinorLocator())\n",
    "        axs[i].grid(False)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for i in range(n_models, len(axs)):\n",
    "        fig.delaxes(axs[i])\n",
    "\n",
    "    # Add overall figure title\n",
    "    fig.suptitle('Comparative Graph of Actual vs Predicted Values of Compressive Strength (CsMPa) \\n (y=x line added for reference) \\n', fontsize=15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_effect_line():\n",
    "    \"\"\" \n",
    "    This function creates an interactive scatter plot of actual vs predicted values for the best model, with a trendline.\n",
    "    Checkboxes are added to the plot, allowing the user to remove variables from the model. The plot is updated when the user clicks a checkbox.\n",
    "    A text box is added to the plot, containing the effect of removing each variable on the error metrics.\n",
    "\n",
    "    Parameters: None\n",
    "    Returns: None (The plot is displayed using `plt.show()`)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of checkboxes, taking the variable names from the global variable `gVarNames`\n",
    "    checkboxes = [widgets.Checkbox(value=True, description=var) for var in gVarNames[0:8]]\n",
    "\n",
    "    def update_lines(*args, **kwargs):\n",
    "        \"\"\"\n",
    "        This function updates the scatter plot when the user clicks a checkbox.\n",
    "\n",
    "        Parameters:\n",
    "        *args: A list of arguments. The first argument is a boolean value indicating whether the checkbox is checked.\n",
    "        **kwargs: A dictionary of keyword arguments. The key is the name of the checkbox, and the value is a boolean indicating whether the checkbox is checked.\n",
    "\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        active_vars = [var.description for var in checkboxes if var.value] # Get the names of the active variables\n",
    "        # Checks that at least one checkbox (variable) is selected, if not an error is printed and the function returns\n",
    "        if not active_vars:\n",
    "            print(\"Error: At least one checkbox must be selected.\")\n",
    "            return\n",
    "        \n",
    "        clear_output(wait=True) # Clear the output of the cell\n",
    "        sns.set_style(\"whitegrid\") # Set the style of the plot\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6)) # Create a figure and axes\n",
    "        plt.size=(10, 6)\n",
    "\n",
    "        fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)  # Center the plot\n",
    "        active_vars = [var.description for var in checkboxes if var.value]\n",
    "        binary_string = ''.join(['1' if not checkbox.value else '0' for checkbox in checkboxes]) # Create a binary string representing the active variables\n",
    "        print(f'Drop Index: {binary_string}')\n",
    "        binary_int = int(binary_string,2)\n",
    "\n",
    "        x = gy_test\n",
    "        y = gVarErrorDf.loc[(binary_int)].iloc[1] # Using the binary string to select the correct row in the global DataFrame `gVarErrorDf`\n",
    "        ax.scatter(x, y, label='Data')  # Add a label to the scatter plot\n",
    "        \n",
    "        # Fit a line to the data\n",
    "        z = np.polyfit(x, y, 1)\n",
    "        p = np.poly1d(z)\n",
    "        \n",
    "        # Add the line to the plot\n",
    "        ax.plot(x, p(x), \"r--\", label='Trendline')  # Add a label to the trendline\n",
    "        \n",
    "        plt.xticks(fontsize=14)  # Increase the size of the x-axis labels\n",
    "        plt.yticks(fontsize=14)  # Increase the size of the y-axis labels\n",
    "        \n",
    "        # Add titles & axis titles\n",
    "        plt.title('Scatter plot of test against predicted values of \\n compressive strength (CsMPa) ', fontsize=16) \n",
    "        plt.xlabel('Actual CsMPa', fontsize=14)\n",
    "        plt.ylabel('Predicted CsMPa', fontsize=14)\n",
    "\n",
    "        # Add legend\n",
    "        plt.legend(bbox_to_anchor=(0.95, 0.2))\n",
    "      \n",
    "        ax = plt.gca() # Set ax = the current axes\n",
    "        plt.minorticks_on()\n",
    "        ax.grid(False)\n",
    "        \n",
    "        \n",
    "        # Adding a text box containing each error metric rounded to 4 decimal places\n",
    "        rmse = round(gVarErrorDf.loc[binary_int].iloc[2], 4)\n",
    "        r2 = round(gVarErrorDf.loc[binary_int].iloc[3], 4)\n",
    "        mae = round(gVarErrorDf.loc[binary_int].iloc[4], 4)\n",
    "        \n",
    "        text_box = f'RMSE = {rmse}\\nR^2 = {r2}\\nMAE = {mae}'\n",
    "        plt.text(0.05, 0.95, text_box, transform=ax.transAxes, verticalalignment='top', fontsize=12, bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5'))\n",
    "        \n",
    "        textstr ='''This scatter plot shows the relationship between the actual and \n",
    "        predicted values of Compressive Strength. A trendline is shown in red. \n",
    "        Each point on the plot represents a data point. The position of\n",
    "        a point on the x-axis represents its actual value, and the position on \n",
    "        the y-axis represents its predicted value. The text box in the top left \n",
    "        corner shows the effect of removing each variable on the error metrics. \n",
    "        The RMSE, R^2, and MAE values are calculated with each variable removed, \n",
    "        and are rounded to 4 decimal places. RMSE is the Root Mean Square Error,\n",
    "        a measure of the differences between values predicted by a model and the \n",
    "        values actually observed.'''\n",
    "        \n",
    "        # Add a text box containing the description of the plot\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        ax.text(1.05, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    # Grid of checkboxes in a 4x2 grid\n",
    "    grid = [\n",
    "        [checkboxes[i] for i in range(j, j+4)]\n",
    "        for j in range(0, 8, 4)\n",
    "    ]\n",
    "\n",
    "    grid = [[checkboxes[i] for i in range(j, j + 4)] for j in range(0, 8, 4)]\n",
    "    ui = widgets.VBox([widgets.HBox(row) for row in grid])\n",
    "    out = widgets.interactive_output(update_lines, {checkbox.description: checkbox for checkbox in checkboxes})\n",
    "\n",
    "    display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_effect_bar():\n",
    "    \"\"\"\n",
    "    This function generates an interactive bar chart, showing the effect of removing each variable on the performance metrics (RMSE, R^2, and MAE).\n",
    "    Data is taken from the global DataFrame `gVarErrorDf`, which is populated by the `var_performance` function.\n",
    "\n",
    "    A dropdown menu is used to select the performance metric to display. The bar chart shows the performance metric for each variable, as well as the baseline (i.e., when no variables are removed).\n",
    "\n",
    "    Parameters:    None\n",
    "    Returns:    None (displays a bar chart)\n",
    "    \"\"\"\n",
    "\n",
    "    # List of binary variable names\n",
    "    index = [bin(2**i) for i in range(8)] #index = ['0b1', '0b10'...\n",
    "    \n",
    "    # List of error types\n",
    "    error_types = ['RMSE', 'R^2', 'MAE']\n",
    "\n",
    "    def plot_bar_chart(error_type):\n",
    "        # Get the values for each variable from gVarErrorDf\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        global gVarNames\n",
    "        values = []\n",
    "        for name in index:\n",
    "            row = gVarErrorDf.loc[gVarErrorDf['Drop index'] == name, error_type]\n",
    "            if len(row) > 0:\n",
    "                values.append(row.values[0])\n",
    "            else:\n",
    "                values.append(np.nan)  # replace None with np.nan\n",
    "        \n",
    "\n",
    "        plt.bar(gVarNames[0:8], values[::-1]) #Corrects order of names vs values, do to drop index being in reverse order\n",
    "        plt.title(f'Bar chart of {error_type} after removing variable')\n",
    "        plt.xlabel('Variable', fontsize=14, labelpad=10)\n",
    "        plt.ylabel(error_type, fontsize=14)\n",
    "\n",
    "        # Add a constant line for the baseline (no variables removed)\n",
    "        baseline = gVarErrorDf.loc[gVarErrorDf['Drop index'] == '0b0', error_type].values[0]\n",
    "        plt.axhline(y=baseline, color='r', linestyle='--')\n",
    "\n",
    "        plt.legend(['Baseline', 'Variable'])\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.grid(False)\n",
    "        ax = plt.gca()\n",
    "        ax.minorticks_on()\n",
    "        \n",
    "        ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "        \n",
    "    \n",
    "        textstr = 'This graph shows the effect of removing each variable on the ' + error_type + '.\\n' \\\n",
    "              'The red dashed line represents the baseline (no variables removed).\\n' \\\n",
    "              'Use the dropdown menu to select the error type.'\n",
    "        props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "        plt.text(1.05, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n",
    "            verticalalignment='top', bbox=props)\n",
    "        plt.show()\n",
    "\n",
    "    # Create dropdown menu for error types\n",
    "    interact(plot_bar_chart, error_type=error_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rf_trees(rf_model, num_trees=1):\n",
    "    \"\"\"\n",
    "    This function visualizes the first 'num_trees' trees in a Random Forest model.\n",
    "\n",
    "    Parameters:\n",
    "    rf_model (sklearn.ensemble.RandomForestRegressor): The trained Random Forest model.\n",
    "    num_trees (int): The number of trees to visualize. Default is 1.\n",
    "    \"\"\"\n",
    "    for i in range(num_trees):\n",
    "        plt.figure(figsize=(20,10))  # Set the figure size\n",
    "        plot_tree(rf_model.estimators_[i], filled=True)  # Plot the i-th decision tree\n",
    "        plt.show()\n",
    "\n",
    "#rf_model, y_pred = random_forest_regression(gX_train_preprocessed, gy_train, gX_test_preprocessed)\n",
    "#visualize_rf_trees(rf_model, num_trees=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: main()\n",
    "\n",
    "The `main()` function is the entry point of the program. It is responsible for coordinating the execution of other functions and controlling the flow of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    X,y = csv_import()\n",
    "    preprocessing(X,y)\n",
    "    print(\"Preprocessing done\")\n",
    "    comp_graph()\n",
    "    best_model, df = regression_performance()\n",
    "    print(\"The best model is: \", best_model)\n",
    "    #hyperparameter \n",
    "    print(\"Performing optimisation via grid search\")\n",
    "    best_grid_params = grid_search()\n",
    "    print(\"Grid search complete: Best parameters:\", best_grid_params, \"\\nErrors are as follows:\")\n",
    "    best_grid_params = opt_unopt_random_forest(best_grid_params)\n",
    "    #var_performance\n",
    "    var_performance(best_grid_params)\n",
    "    gVarErrorDf.to_csv('var_performance.csv')\n",
    "    print (gVarErrorDf)\n",
    "    var_impact_pct()\n",
    "    #var_drop_final()\n",
    "    #print(\"Var performance done\") \n",
    "    var_effect_line() \n",
    "    var_effect_bar()\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
